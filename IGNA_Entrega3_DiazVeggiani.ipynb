{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1Cu0-pz1eW"
      },
      "source": [
        "# Tercera entrega - Trabajo Integrador - **Grupo 17 5k10**\n",
        "##  **Integrantes:** Veggiani Franco, Diaz Juan Ignacio\n",
        "\n",
        "### Enunciado:\n",
        "En esta entrega se espera:\n",
        "1. Que cada grupo defina claramente el objetivo predictivo del proyecto, identificando con precisi√≥n la variable objetivo y el tipo de problema (clasificaci√≥n, regresi√≥n, etc.).\n",
        "2. El an√°lisis debe incluir la estrategia de partici√≥n del conjunto de datos (entrenamiento/test o validaci√≥n cruzada), y la construcci√≥n de un pipeline completo de Scikit-learn que integre todas las etapas necesarias: preprocesamiento, modelado y evaluaci√≥n.\n",
        "3. Deber√°n comparar al menos tres modelos distintos y justificar cu√°l se ajusta mejor al problema.\n",
        "4. Luego, deber√°n realizar un proceso de b√∫squeda y ajuste de hiperpar√°metros sobre el modelo elegido, utilizando t√©cnicas como GridSearchCV o RandomizedSearchCV.\n",
        "5. Se espera una evaluaci√≥n rigurosa con m√©tricas adecuadas (accuracy, F1, RMSE, etc., seg√∫n el caso), as√≠ como un diagn√≥stico de overfitting o underfitting y posibles acciones para mitigarlos.\n",
        "\n",
        "La entrega debe presentarse como un Jupyter Notebook claro, bien estructurado y reproducible, y los integrantes del grupo deben poder explicar oralmente las decisiones tomadas durante todo el proceso de modelado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULmiFYOgC9Zp"
      },
      "source": [
        "## 0. Definici√≥n del pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pzQpHhMDDfhu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pG9D_Qn2dGyI"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ALTERNATIVA A LAGS: FEATURES AGREGADAS HIST√ìRICAS\n",
        "# ============================================================================\n",
        "# En lugar de usar lag_1, lag_7, etc., usamos estad√≠sticas agregadas\n",
        "# por grupo que se calculan UNA VEZ y se guardan como \"lookup table\"\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HistoricalProfileEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica el \"perfil hist√≥rico\" de cada l√≠nea-municipio-d√≠a_semana.\n",
        "\n",
        "    En ENTRENAMIENTO:\n",
        "    - Calcula estad√≠sticas agregadas por grupo\n",
        "\n",
        "    En PREDICCI√ìN:\n",
        "    - Usa las estad√≠sticas pre-calculadas (NO necesita historial reciente)\n",
        "\n",
        "    Ejemplo:\n",
        "    --------\n",
        "    Para predecir la l√≠nea 740 en San Miguel un Lunes:\n",
        "    - NO usa cu√°ntos pasajeros hubo ayer (lag_1) ‚úó\n",
        "    - S√ç usa \"los lunes en esta l√≠nea suelen haber X pasajeros\" ‚úì\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, group_cols=['linea', 'municipio', 'dia_semana']):\n",
        "        self.group_cols = group_cols\n",
        "        self.profiles = {}\n",
        "        self.global_stats = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula perfiles hist√≥ricos desde los datos de entrenamiento\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüìä Calculando perfiles hist√≥ricos...\")\n",
        "\n",
        "        # 1. Estad√≠sticas por grupo (l√≠nea + municipio + d√≠a de semana)\n",
        "        group_stats = df.groupby(self.group_cols)['cantidad'].agg([\n",
        "            'mean',    # Promedio hist√≥rico\n",
        "            'std',     # Variabilidad\n",
        "            'median',  # Mediana (robusto a outliers)\n",
        "            'min',     # M√≠nimo hist√≥rico\n",
        "            'max',     # M√°ximo hist√≥rico\n",
        "            'count'    # Cantidad de observaciones\n",
        "        ]).reset_index()\n",
        "\n",
        "        self.profiles['main'] = group_stats\n",
        "\n",
        "        # 2. Estad√≠sticas por l√≠nea + municipio (sin d√≠a de semana)\n",
        "        line_muni_stats = df.groupby(['linea', 'municipio'])['cantidad'].agg([\n",
        "            'mean', 'std', 'median'\n",
        "        ]).reset_index()\n",
        "        line_muni_stats.columns = ['linea', 'municipio', 'lm_mean', 'lm_std', 'lm_median']\n",
        "        self.profiles['line_muni'] = line_muni_stats\n",
        "\n",
        "        # 3. Estad√≠sticas por l√≠nea + d√≠a de semana\n",
        "        line_day_stats = df.groupby(['linea', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        line_day_stats.columns = ['linea', 'dia_semana', 'ld_mean', 'ld_std']\n",
        "        self.profiles['line_day'] = line_day_stats\n",
        "\n",
        "        # 4. Estad√≠sticas por municipio + d√≠a de semana\n",
        "        muni_day_stats = df.groupby(['municipio', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        muni_day_stats.columns = ['municipio', 'dia_semana', 'md_mean', 'md_std']\n",
        "        self.profiles['muni_day'] = muni_day_stats\n",
        "\n",
        "        # 5. Estad√≠sticas globales (fallback)\n",
        "        self.global_stats = {\n",
        "            'mean': df['cantidad'].mean(),\n",
        "            'std': df['cantidad'].std(),\n",
        "            'median': df['cantidad'].median()\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úì {len(group_stats)} perfiles √∫nicos creados\")\n",
        "        print(f\"   ‚úì Cobertura: {(group_stats['count'].sum() / len(df)) * 100:.1f}%\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features basadas en perfiles hist√≥ricos\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Merge con estad√≠sticas principales\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['main'],\n",
        "            on=self.group_cols,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas l√≠nea-municipio\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['line_muni'],\n",
        "            on=['linea', 'municipio'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas l√≠nea-d√≠a\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['line_day'],\n",
        "            on=['linea', 'dia_semana'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas municipio-d√≠a\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['muni_day'],\n",
        "            on=['municipio', 'dia_semana'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Rellenar valores faltantes con estad√≠sticas globales\n",
        "        fill_cols = ['mean', 'std', 'median', 'lm_mean', 'lm_std', 'lm_median',\n",
        "                     'ld_mean', 'ld_std', 'md_mean', 'md_std']\n",
        "\n",
        "        for col in fill_cols:\n",
        "            if col in X_new.columns:\n",
        "                base_stat = 'mean' if 'mean' in col else 'std' if 'std' in col else 'median'\n",
        "                X_new[col].fillna(self.global_stats[base_stat], inplace=True)\n",
        "\n",
        "        # Features derivadas\n",
        "        X_new['volatility'] = X_new['std'] / (X_new['mean'] + 1)  # Coeficiente de variaci√≥n\n",
        "        X_new['normalized_demand'] = X_new['mean'] / (X_new['lm_mean'] + 1)  # Demanda relativa\n",
        "\n",
        "        return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "V_KTlp6BdLIO"
      },
      "outputs": [],
      "source": [
        "class WeatherImpactEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica c√≥mo el clima afecta hist√≥ricamente a cada l√≠nea-municipio.\n",
        "\n",
        "    Ejemplo:\n",
        "    --------\n",
        "    - Algunas l√≠neas son muy sensibles a lluvia (zonas sin alternativas)\n",
        "    - Otras no tanto (zonas c√©ntricas con m√∫ltiples opciones)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weather_impacts = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula sensibilidad al clima por grupo\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüå¶Ô∏è Calculando impacto del clima...\")\n",
        "\n",
        "        for group in df.groupby(['linea', 'municipio']):\n",
        "            key = group[0]\n",
        "            data = group[1]\n",
        "\n",
        "            if len(data) < 10:  # Muy pocos datos\n",
        "                continue\n",
        "\n",
        "            # Correlaci√≥n entre lluvia y demanda\n",
        "            rain_corr = data[['precip', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            # Diferencia de demanda en d√≠as lluviosos vs secos\n",
        "            rainy_days = data[data['precip'] > 5]['cantidad'].mean()\n",
        "            dry_days = data[data['precip'] <= 5]['cantidad'].mean()\n",
        "            rain_impact = (rainy_days - dry_days) / dry_days if dry_days > 0 else 0\n",
        "\n",
        "            # Sensibilidad a temperatura\n",
        "            temp_corr = data[['t_med', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            self.weather_impacts[key] = {\n",
        "                'rain_correlation': rain_corr,\n",
        "                'rain_impact_pct': rain_impact,\n",
        "                'temp_correlation': temp_corr\n",
        "            }\n",
        "\n",
        "        print(f\"   ‚úì {len(self.weather_impacts)} perfiles clim√°ticos creados\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features de impacto clim√°tico\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Inicializar columnas\n",
        "        X_new['rain_sensitivity'] = 0.0\n",
        "        X_new['rain_impact'] = 0.0\n",
        "        X_new['temp_sensitivity'] = 0.0\n",
        "\n",
        "        # Aplicar perfiles\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'])\n",
        "\n",
        "            if key in self.weather_impacts:\n",
        "                impacts = self.weather_impacts[key]\n",
        "                X_new.loc[idx, 'rain_sensitivity'] = impacts['rain_correlation']\n",
        "                X_new.loc[idx, 'rain_impact'] = impacts['rain_impact_pct']\n",
        "                X_new.loc[idx, 'temp_sensitivity'] = impacts['temp_correlation']\n",
        "\n",
        "        # Interacciones clima √ó sensibilidad\n",
        "        X_new['adjusted_rain'] = X_new['precip'] * X_new['rain_sensitivity']\n",
        "        X_new['adjusted_temp'] = X_new['t_med'] * X_new['temp_sensitivity']\n",
        "\n",
        "        return X_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "taIRBEUmdN7Q"
      },
      "outputs": [],
      "source": [
        "class SeasonalityEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica patrones estacionales (mensuales/semanales) por grupo.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.seasonal_patterns = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula factores estacionales\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüìÖ Calculando patrones estacionales...\")\n",
        "\n",
        "        # Por l√≠nea-municipio-mes\n",
        "        monthly = df.groupby(['linea', 'municipio', 'mes'])['cantidad'].mean()\n",
        "\n",
        "        # Normalizar por promedio anual de cada grupo\n",
        "        for (linea, municipio) in df.groupby(['linea', 'municipio']).groups.keys():\n",
        "            subset = monthly.loc[linea, municipio]\n",
        "            if len(subset) > 0:\n",
        "                annual_avg = subset.mean()\n",
        "                if annual_avg > 0:\n",
        "                    for mes in subset.index:\n",
        "                        key = (linea, municipio, mes)\n",
        "                        self.seasonal_patterns[key] = subset[mes] / annual_avg\n",
        "\n",
        "        print(f\"   ‚úì {len(self.seasonal_patterns)} patrones estacionales\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega factor estacional\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "        X_new['seasonal_factor'] = 1.0\n",
        "\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'], row['mes'])\n",
        "            if key in self.seasonal_patterns:\n",
        "                X_new.loc[idx, 'seasonal_factor'] = self.seasonal_patterns[key]\n",
        "\n",
        "        return X_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3_nxl0cbFpiX"
      },
      "outputs": [],
      "source": [
        "class DateSorter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Convierte y ordena por fecha. Debe ser el primer paso del pipeline.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='fecha'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Convertir fecha a datetime si no lo est√°\n",
        "        if not pd.api.types.is_datetime64_any_dtype(X[self.date_column]):\n",
        "            X[self.date_column] = pd.to_datetime(X[self.date_column])\n",
        "\n",
        "        # Ordenar por fecha\n",
        "        X = X.sort_values(self.date_column).reset_index(drop=True)\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "fFhPK50DDBUU"
      },
      "outputs": [],
      "source": [
        "class TemporalFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extrae features temporales de la columna fecha y crea features c√≠clicas.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='fecha'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # La fecha ya debe estar como datetime gracias a DateSorter\n",
        "\n",
        "        # Extraer features temporales b√°sicas\n",
        "        X['dia_semana'] = X[self.date_column].dt.dayofweek\n",
        "        X['mes'] = X[self.date_column].dt.month\n",
        "        X['is_weekend'] = X['dia_semana'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "        # Features c√≠clicas para mes\n",
        "        X['mes_sin'] = np.sin(2 * np.pi * X['mes'] / 12)\n",
        "        X['mes_cos'] = np.cos(2 * np.pi * X['mes'] / 12)\n",
        "\n",
        "        # Features c√≠clicas para d√≠a de semana\n",
        "        X['dia_sin'] = np.sin(2 * np.pi * X['dia_semana'] / 7)\n",
        "        X['dia_cos'] = np.cos(2 * np.pi * X['dia_semana'] / 7)\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8q4SEaf3DFSL"
      },
      "outputs": [],
      "source": [
        "class TemperatureFeatureCreator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Crea features derivadas de temperatura.\n",
        "    \"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Temperatura media\n",
        "        X['t_med'] = (X['tmax'] + X['tmin']) / 2\n",
        "\n",
        "        # Amplitud t√©rmica\n",
        "        X['t_amp'] = X['tmax'] - X['tmin']\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "D2l07osnKHwQ"
      },
      "outputs": [],
      "source": [
        "class DropNaRows(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Elimina filas que tengan NaN en columnas espec√≠ficas.\n",
        "    Pensado para usarse luego de LagFeatureCreator, antes del split X/y.\n",
        "    \"\"\"\n",
        "    def __init__(self, columns=None, how='any'):\n",
        "        self.columns = columns or []\n",
        "        self.how = how  # 'any' o 'all'\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if self.columns:\n",
        "            X = X.dropna(subset=self.columns, how=self.how)\n",
        "        X = X.reset_index(drop=True)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Sasya0rdDH-P"
      },
      "outputs": [],
      "source": [
        "class LagFeatureCreator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Crea features de lag y rolling para series temporales.\n",
        "    Agrupa por l√≠nea y municipio para mantener la coherencia temporal.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_col='cantidad', date_col='fecha',\n",
        "                 group_cols=['linea', 'municipio'],\n",
        "                 lags=[1, 7, 28],\n",
        "                 rolling_windows=[7, 28]):\n",
        "        self.target_col = target_col\n",
        "        self.date_col = date_col\n",
        "        self.group_cols = group_cols\n",
        "        self.lags = lags\n",
        "        self.rolling_windows = rolling_windows\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Asegurar que fecha sea datetime y ordenar\n",
        "        if not pd.api.types.is_datetime64_any_dtype(X[self.date_col]):\n",
        "            X[self.date_col] = pd.to_datetime(X[self.date_col])\n",
        "\n",
        "        X = X.sort_values(self.date_col).reset_index(drop=True)\n",
        "\n",
        "        def add_lags_rolls(g):\n",
        "            g = g.sort_values(self.date_col)\n",
        "\n",
        "            # Crear lags\n",
        "            for lag in self.lags:\n",
        "                g[f'lag_{lag}'] = g[self.target_col].shift(lag)\n",
        "\n",
        "            # # Crear rolling means\n",
        "            # for window in self.rolling_windows:\n",
        "            #     g[f'roll_{window}'] = g[self.target_col].shift(1).rolling(\n",
        "            #         window=window, min_periods=1\n",
        "            #     ).mean()\n",
        "\n",
        "            # Indicadores de lag faltante (opcional)\n",
        "            for lag in self.lags:\n",
        "                g[f'has_lag_{lag}'] = g[f'lag_{lag}'].isnull().astype(int)\n",
        "\n",
        "            return g\n",
        "\n",
        "        # Aplicar por grupo\n",
        "        X = X.groupby(self.group_cols, group_keys=False).apply(\n",
        "            add_lags_rolls\n",
        "        )\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "J4G_zb0ZDJ5K"
      },
      "outputs": [],
      "source": [
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Elimina columnas espec√≠ficas del DataFrame.\n",
        "    \"\"\"\n",
        "    def __init__(self, columns_to_drop=None):\n",
        "        self.columns_to_drop = columns_to_drop or []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X.drop(columns=self.columns_to_drop, errors='ignore')\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Oeaq5FymDSUN"
      },
      "outputs": [],
      "source": [
        "class Winsorizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Aplica winsorizaci√≥n a columnas espec√≠ficas para manejar outliers.\n",
        "    \"\"\"\n",
        "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99, columns=None):\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "        self.columns = columns\n",
        "        self.lower_bounds = {}\n",
        "        self.upper_bounds = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.columns is None:\n",
        "            self.columns = X.columns\n",
        "\n",
        "        for col in self.columns:\n",
        "            if col in X.columns:\n",
        "                self.lower_bounds[col] = np.percentile(\n",
        "                    X[col].dropna(), self.lower_percentile * 100\n",
        "                )\n",
        "                self.upper_bounds[col] = np.percentile(\n",
        "                    X[col].dropna(), self.upper_percentile * 100\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        for col in self.columns:\n",
        "            if col in X_transformed.columns:\n",
        "                X_transformed[col] = np.clip(\n",
        "                    X_transformed[col],\n",
        "                    self.lower_bounds[col],\n",
        "                    self.upper_bounds[col]\n",
        "                )\n",
        "        return X_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "t2HGGqRmKNnx"
      },
      "outputs": [],
      "source": [
        "def create_fe_pipeline(\n",
        "    target_col='cantidad',\n",
        "    date_col='fecha',\n",
        "    group_cols=['linea', 'municipio'],\n",
        "    lags=[28],\n",
        "    rolling_windows=[7, 28],\n",
        "    dropna_lag_cols=('cantidad', 'lag_1', 'lag_7', 'lag_28', 'roll_7', 'roll_28')\n",
        "):\n",
        "    return Pipeline([\n",
        "        (\"date_sorter\", DateSorter(date_column='fecha')),\n",
        "        (\"temporal_features\", TemporalFeatureExtractor(date_column='fecha')),\n",
        "        (\"temperature_features\", TemperatureFeatureCreator()),\n",
        "        (\"lag_features\", LagFeatureCreator(lags=lags, group_cols=group_cols))\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wV5xFU_kL4Gl"
      },
      "outputs": [],
      "source": [
        "# Recrear el pipeline final sin lambdas\n",
        "def create_final_preprocessing_pipeline(\n",
        "    columns_to_drop=['fecha', 'provincia', 'nombre_feriado', 'tipo_transporte', 'tipo_feriado', 'linea', 'empresa'],\n",
        "    winsorize_cols=['t_med', 'precip', 'tmax', 'tmin']\n",
        "):\n",
        "    \"\"\"\n",
        "    Versi√≥n serializable del pipeline final.\n",
        "    Usa make_column_selector en lugar de lambdas.\n",
        "    \"\"\"\n",
        "    numeric_pipeline = Pipeline([\n",
        "        (\"winsorizer\", Winsorizer(columns=winsorize_cols)),\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"drop_columns\", DropColumns(columns_to_drop=columns_to_drop)),\n",
        "        (\"column_transform\", ColumnTransformer([\n",
        "            (\"num\", numeric_pipeline, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
        "            (\"cat\", categorical_pipeline, make_column_selector(dtype_include=[\"object\", \"category\"]))\n",
        "        ]))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsUuzCZBIBYk"
      },
      "source": [
        "## 1. Objetivo predictivo del proyecto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iGnm8lxIFAR"
      },
      "source": [
        "- **Objetivo del proyecto**:  Predecir la cantidad de pasajeros transportados por una l√≠nea de colectivo en una fecha determinada, utilizando informaci√≥n contextual como clima, feriados, ubicaci√≥n y caracter√≠sticas del servicio.\n",
        "\n",
        "  - **Variable objetivo**: `cantidad` (n√∫mero entero que representa la cantidad de pasajeros)\n",
        "\n",
        "  - **Tipo de problema**: `Regresi√≥n`, ya que se busca predecir un valor num√©rico continuo.\n",
        "\n",
        "- **Justificaci√≥n**: La variable `cantidad` no representa clases ni categor√≠as, sino un conteo que var√≠a en funci√≥n de m√∫ltiples factores. Por lo tanto, se trata de un problema de regresi√≥n supervisada.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLsCywfYpjde"
      },
      "source": [
        "## 2. An√°lisis del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW6aKxdQ7cJc",
        "outputId": "d666bae7-a90f-48df-d233-4cf73dfd3eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- (FILAS, COLUMNAS) -----\n",
            "(133204, 14)\n",
            "---- (TIPOS DE DATOS) -----\n",
            "fecha               object\n",
            "empresa             object\n",
            "linea               object\n",
            "tipo_transporte     object\n",
            "provincia           object\n",
            "municipio           object\n",
            "cantidad             int64\n",
            "tmax               float64\n",
            "tmin               float64\n",
            "precip             float64\n",
            "viento             float64\n",
            "is_feriado           int64\n",
            "tipo_feriado        object\n",
            "nombre_feriado      object\n",
            "dtype: object\n",
            "---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\n",
            "count    133204.000000\n",
            "mean      14055.787386\n",
            "std       19946.489535\n",
            "min           1.000000\n",
            "25%        2169.000000\n",
            "50%        6973.000000\n",
            "75%       17665.000000\n",
            "max      189636.000000\n",
            "Name: cantidad, dtype: float64\n",
            "---- (NULOS) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia              22\n",
            "municipio              22\n",
            "cantidad                0\n",
            "tmax                 2137\n",
            "tmin                 2137\n",
            "precip               2137\n",
            "viento               2137\n",
            "is_feriado              0\n",
            "tipo_feriado       126287\n",
            "nombre_feriado     126287\n",
            "dtype: int64\n",
            "---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\n",
            "(131067, 14)\n",
            "---- (NULOS DESPU√âS DE DROPEAR NAN) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia               0\n",
            "municipio               0\n",
            "cantidad                0\n",
            "tmax                    0\n",
            "tmin                    0\n",
            "precip                  0\n",
            "viento                  0\n",
            "is_feriado              0\n",
            "tipo_feriado       124210\n",
            "nombre_feriado     124210\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('final_2024-11-04.csv')\n",
        "print(\"---- (FILAS, COLUMNAS) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (TIPOS DE DATOS) -----\")\n",
        "print(df.dtypes)\n",
        "print(\"---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\")\n",
        "print(df['cantidad'].describe())\n",
        "print(\"---- (NULOS) -----\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with NaN in specified columns\n",
        "df = df.dropna(subset=['tmax', 'tmin', 'precip', 'viento'], how='any').reset_index(drop=True)\n",
        "\n",
        "print(\"---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (NULOS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNkE4hcMppD7"
      },
      "source": [
        "###  2.1 Preprocesamiento y pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6HV76Hz_Sn_"
      },
      "source": [
        "**MUY IMPORTANTE ESTO** Se obtiene el dia y el mes de la semana, porque la fecha como tal no sirve para el entrenamiento, no se puede generalizar.\n",
        "\n",
        "Debemos obtener tambi√©n el n√∫mero de d√≠a en la semana (0 - 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- (FILAS, COLUMNAS) -----\n",
            "(133204, 14)\n",
            "---- (TIPOS DE DATOS) -----\n",
            "fecha               object\n",
            "empresa             object\n",
            "linea               object\n",
            "tipo_transporte     object\n",
            "provincia           object\n",
            "municipio           object\n",
            "cantidad             int64\n",
            "tmax               float64\n",
            "tmin               float64\n",
            "precip             float64\n",
            "viento             float64\n",
            "is_feriado           int64\n",
            "tipo_feriado        object\n",
            "nombre_feriado      object\n",
            "dtype: object\n",
            "---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\n",
            "count    133204.000000\n",
            "mean      14055.787386\n",
            "std       19946.489535\n",
            "min           1.000000\n",
            "25%        2169.000000\n",
            "50%        6973.000000\n",
            "75%       17665.000000\n",
            "max      189636.000000\n",
            "Name: cantidad, dtype: float64\n",
            "---- (NULOS) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia              22\n",
            "municipio              22\n",
            "cantidad                0\n",
            "tmax                 2137\n",
            "tmin                 2137\n",
            "precip               2137\n",
            "viento               2137\n",
            "is_feriado              0\n",
            "tipo_feriado       126287\n",
            "nombre_feriado     126287\n",
            "dtype: int64\n",
            "---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\n",
            "(131067, 14)\n",
            "---- (NULOS DESPU√âS DE DROPEAR NAN) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia               0\n",
            "municipio               0\n",
            "cantidad                0\n",
            "tmax                    0\n",
            "tmin                    0\n",
            "precip                  0\n",
            "viento                  0\n",
            "is_feriado              0\n",
            "tipo_feriado       124210\n",
            "nombre_feriado     124210\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('final_2024-11-04.csv')\n",
        "print(\"---- (FILAS, COLUMNAS) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (TIPOS DE DATOS) -----\")\n",
        "print(df.dtypes)\n",
        "print(\"---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\")\n",
        "print(df['cantidad'].describe())\n",
        "print(\"---- (NULOS) -----\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with NaN in specified columns\n",
        "df = df.dropna(subset=['tmax', 'tmin', 'precip', 'viento'], how='any').reset_index(drop=True)\n",
        "\n",
        "print(\"---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (NULOS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "J2OpOcauIrNa"
      },
      "outputs": [],
      "source": [
        "preprocessor = create_fe_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_139606/633612192.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  X = X.groupby(self.group_cols, group_keys=False).apply(\n"
          ]
        }
      ],
      "source": [
        "df_fe = preprocessor.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Split temporal 70/15/15 (la columna 'fecha' ya es datetime y est√° ordenada)\n",
        "cut_train = df_fe[\"fecha\"].quantile(0.70)\n",
        "cut_valid = df_fe[\"fecha\"].quantile(0.85)\n",
        "\n",
        "train_mask = df_fe[\"fecha\"] <= cut_train\n",
        "valid_mask = (df_fe[\"fecha\"] > cut_train) & (df_fe[\"fecha\"] <= cut_valid)\n",
        "test_mask  = df_fe[\"fecha\"] > cut_valid\n",
        "\n",
        "df_train = df_fe[train_mask].copy()\n",
        "df_valid = df_fe[valid_mask].copy()\n",
        "df_test  = df_fe[test_mask].copy()\n",
        "\n",
        "# 5) Separar X e y (reci√©n ahora sacamos la target de X)\n",
        "X_train, y_train = df_train.drop(columns=[\"cantidad\"]), df_train[\"cantidad\"]\n",
        "X_valid, y_valid = df_valid.drop(columns=[\"cantidad\"]), df_valid[\"cantidad\"]\n",
        "X_test,  y_test  = df_test.drop(columns=[\"cantidad\"]),  df_test[\"cantidad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Preprocesamiento final (ONEHOT/SCALER) reci√©n ahora\n",
        "final_pre = create_final_preprocessing_pipeline()\n",
        "X_train_transformed = final_pre.fit_transform(X_train)\n",
        "X_valid_transformed = final_pre.transform(X_valid)\n",
        "X_test_transformed  = final_pre.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnnQxmPUp0YV"
      },
      "source": [
        "## 3. Comparaci√≥n de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y2kB8fFp3V4"
      },
      "source": [
        "### 3.1 Evaluaci√≥n con m√©tricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "R6TvWvw6HKJN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "MOBScn-2HQ7A"
      },
      "outputs": [],
      "source": [
        "def safe_mape(y_true, y_pred, eps=1e-6):\n",
        "    \"\"\"Calcula MAPE de forma segura evitando divisi√≥n por cero\"\"\"\n",
        "    denom = np.where(np.abs(y_true) < eps, eps, y_true)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, dataset_name):\n",
        "    \"\"\"Calcula todas las m√©tricas relevantes\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = safe_mape(np.array(y_true), np.array(y_pred))\n",
        "\n",
        "    return {\n",
        "        'Dataset': dataset_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R¬≤': r2,\n",
        "        'MAPE (%)': mape\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSsn415sHiMo",
        "outputId": "783423e3-3bff-4a8a-ff3b-6e616e0eee93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Modelos a comparar:\n",
            "   1. Linear Regression (Baseline)\n",
            "   2. Random Forest\n",
            "   3. Gradient Boosting\n",
            "   4. XGBoost (con configuraci√≥n optimizada)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìã Modelos a comparar:\")\n",
        "print(\"   1. Linear Regression (Baseline)\")\n",
        "print(\"   2. Random Forest\")\n",
        "print(\"   3. Gradient Boosting\")\n",
        "print(\"   4. XGBoost (con configuraci√≥n optimizada)\")\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    'XGBoost': XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.01,\n",
        "        max_depth=8,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=10,\n",
        "        random_state=42,\n",
        "        early_stopping_rounds=50,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3GLPHoNHvgY",
        "outputId": "c31914d8-4de1-433e-be45-bb4dec34b8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÑ Entrenando y evaluando modelos...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚ñ∂ Entrenando Linear Regression...\n",
            "  ‚úì Entrenamiento completado en 0.59 segundos\n",
            "  üìä Valid RMSE: 6854.92 | R¬≤: 0.8711\n",
            "\n",
            "‚ñ∂ Entrenando Random Forest...\n",
            "  ‚úì Entrenamiento completado en 14.36 segundos\n",
            "  üìä Valid RMSE: 5989.52 | R¬≤: 0.9016\n",
            "\n",
            "‚ñ∂ Entrenando Gradient Boosting...\n",
            "  ‚úì Entrenamiento completado en 24.98 segundos\n",
            "  üìä Valid RMSE: 5926.33 | R¬≤: 0.9037\n",
            "\n",
            "‚ñ∂ Entrenando XGBoost...\n",
            "  ‚úì Entrenamiento completado en 10.30 segundos\n",
            "  üìä Valid RMSE: 5375.13 | R¬≤: 0.9207\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîÑ Entrenando y evaluando modelos...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "all_results = []\n",
        "training_times = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n‚ñ∂ Entrenando {name}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    if name == 'XGBoost':\n",
        "        # XGBoost con early stopping\n",
        "        model.fit(\n",
        "            X_train_transformed,\n",
        "            y_train,\n",
        "            eval_set=[(X_train_transformed, y_train), (X_valid_transformed, y_valid)],\n",
        "            verbose=False\n",
        "        )\n",
        "    else:\n",
        "        model.fit(X_train_transformed, y_train)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    training_times[name] = training_time\n",
        "\n",
        "    print(f\"  ‚úì Entrenamiento completado en {training_time:.2f} segundos\")\n",
        "\n",
        "    # Generar predicciones\n",
        "    y_train_pred = model.predict(X_train_transformed)\n",
        "    y_valid_pred = model.predict(X_valid_transformed)\n",
        "    y_test_pred = model.predict(X_test_transformed)\n",
        "\n",
        "    # Calcular m√©tricas para cada conjunto\n",
        "    metrics_train = calculate_metrics(y_train, y_train_pred, f'{name} - Train')\n",
        "    metrics_valid = calculate_metrics(y_valid, y_valid_pred, f'{name} - Valid')\n",
        "    metrics_test = calculate_metrics(y_test, y_test_pred, f'{name} - Test')\n",
        "\n",
        "    # Agregar a resultados\n",
        "    all_results.extend([metrics_train, metrics_valid, metrics_test])\n",
        "\n",
        "    # Mostrar m√©tricas del modelo\n",
        "    print(f\"  üìä Valid RMSE: {metrics_valid['RMSE']:.2f} | R¬≤: {metrics_valid['R¬≤']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0zipR8JIKFn",
        "outputId": "73cd75f4-6aba-4647-f5b4-f4c3fc59b9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTADOS COMPLETOS - TODOS LOS MODELOS\n",
            "================================================================================\n",
            "                  Dataset        RMSE         MAE       R¬≤    MAPE (%)\n",
            "Linear Regression - Train 6436.355635 3418.073183 0.899096  471.261597\n",
            "Linear Regression - Valid 6854.919129 3702.255801 0.871092 1032.782618\n",
            " Linear Regression - Test 4994.746077 2745.750607 0.936862  208.167958\n",
            "    Random Forest - Train 4671.408672 2353.217441 0.946847  304.985515\n",
            "    Random Forest - Valid 5989.519684 2874.336098 0.901585  856.128353\n",
            "     Random Forest - Test 4066.122134 2043.884749 0.958157  108.974916\n",
            "Gradient Boosting - Train 5051.883287 2499.444906 0.937837  329.199361\n",
            "Gradient Boosting - Valid 5926.327191 2900.547601 0.903651  973.392429\n",
            " Gradient Boosting - Test 4091.273351 2058.502448 0.957638  125.699994\n",
            "          XGBoost - Train 4288.780013 2242.310303 0.955198  273.476616\n",
            "          XGBoost - Valid 5375.127161 2711.464355 0.920740  935.572639\n",
            "           XGBoost - Test 3802.638295 2017.472168 0.963404  123.261358\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS COMPLETOS - TODOS LOS MODELOS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25-O6FdXIKz5",
        "outputId": "71bab526-ffde-4e3f-c48a-c5b9d8581131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARACI√ìN RESUMIDA (VALIDATION SET)\n",
            "================================================================================\n",
            "           Modelo        RMSE         MAE       R¬≤    MAPE (%)\n",
            "          XGBoost 5375.127161 2711.464355 0.920740  935.572639\n",
            "Gradient Boosting 5926.327191 2900.547601 0.903651  973.392429\n",
            "    Random Forest 5989.519684 2874.336098 0.901585  856.128353\n",
            "Linear Regression 6854.919129 3702.255801 0.871092 1032.782618\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN RESUMIDA (VALIDATION SET)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "valid_results = results_df[results_df['Dataset'].str.contains('Valid')].copy()\n",
        "valid_results['Modelo'] = valid_results['Dataset'].str.replace(' - Valid', '')\n",
        "valid_results = valid_results[['Modelo', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)']].sort_values('RMSE')\n",
        "\n",
        "print(valid_results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2sPzMckINho",
        "outputId": "ceeab37d-a4f3-4e0f-df4f-6bddb2eafda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÜ MEJOR MODELO: XGBoost\n",
            "   RMSE: 5375.13\n",
            "   R¬≤: 0.9207\n",
            "   Tiempo de entrenamiento: 10.30 segundos\n"
          ]
        }
      ],
      "source": [
        "best_model_name = valid_results.iloc[0]['Modelo']\n",
        "best_rmse = valid_results.iloc[0]['RMSE']\n",
        "best_r2 = valid_results.iloc[0]['R¬≤']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   RMSE: {best_rmse:.2f}\")\n",
        "print(f\"   R¬≤: {best_r2:.4f}\")\n",
        "print(f\"   Tiempo de entrenamiento: {training_times[best_model_name]:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBPj6eANRI3"
      },
      "source": [
        "# CONCLUSI√ìN DEL MODELO QUE VAMOS A USAR\n",
        "\n",
        "Vamos a usar el modelo de RANDOM FOREST por el MAPE, ya que es muy inferior al de XGBOOST.\n",
        "\n",
        "Si bien, el XGBoost tiene mejores RMSE y R2 (predice mejor casos donde el error es costoso) su MAPE es muy alto, por lo que falla mucho en errores t√≠picos.\n",
        "\n",
        "En cambio, Random Forest tiene un RMSE un poco menor, pero tiene mejor predicci√≥n ante errores t√≠picos, por lo que se lo considera m√°s \"estable\".\n",
        "\n",
        "## POST AJUSTES OVERFITTING / UNDERFITTING\n",
        "\n",
        "Despu√©s de pruebas de overfitting/underfitting se ajustaron campos como el max_depth del Random forest para que finalmente est√© balanceado y adem√°s sea el mejor modelo. El XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ7-QYVKIPqd",
        "outputId": "a76fbda6-b906-4b3d-f1b4-b15fb345586b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DIAGN√ìSTICO DE OVERFITTING - XGBoost\n",
            "================================================================================\n",
            "üìä M√âTRICAS:\n",
            "   Train RMSE: 4288.78 | R¬≤: 0.9552\n",
            "   Valid RMSE: 5375.13 | R¬≤: 0.9207\n",
            "   Test  RMSE: 3802.64  | R¬≤: 0.9634\n",
            "\n",
            "üîç AN√ÅLISIS:\n",
            "   Diferencia Train-Valid RMSE: -1086.35\n",
            "   Diferencia Valid-Train R¬≤: -0.0345\n",
            "\n",
            "   ‚ùå OVERFITTING SEVERO\n",
            "   üí° Soluciones:\n",
            "      - Aumentar regularizaci√≥n\n",
            "      - Reducir learning_rate\n",
            "      - Reducir max_depth\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"DIAGN√ìSTICO DE OVERFITTING - {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_train_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Train'].iloc[0]\n",
        "best_valid_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Valid'].iloc[0]\n",
        "best_test_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Test'].iloc[0]\n",
        "\n",
        "train_rmse = best_train_metrics['RMSE']\n",
        "valid_rmse = best_valid_metrics['RMSE']\n",
        "test_rmse = best_test_metrics['RMSE']\n",
        "\n",
        "train_r2 = best_train_metrics['R¬≤']\n",
        "valid_r2 = best_valid_metrics['R¬≤']\n",
        "test_r2 = best_test_metrics['R¬≤']\n",
        "\n",
        "overfit_rmse = train_rmse - valid_rmse\n",
        "overfit_r2 = valid_r2 - train_r2\n",
        "\n",
        "print(f\"üìä M√âTRICAS:\")\n",
        "print(f\"   Train RMSE: {train_rmse:.2f} | R¬≤: {train_r2:.4f}\")\n",
        "print(f\"   Valid RMSE: {valid_rmse:.2f} | R¬≤: {valid_r2:.4f}\")\n",
        "print(f\"   Test  RMSE: {test_rmse:.2f}  | R¬≤: {test_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nüîç AN√ÅLISIS:\")\n",
        "print(f\"   Diferencia Train-Valid RMSE: {overfit_rmse:+.2f}\")\n",
        "print(f\"   Diferencia Valid-Train R¬≤: {overfit_r2:+.4f}\")\n",
        "\n",
        "# Diagn√≥stico autom√°tico\n",
        "if overfit_rmse < -500:\n",
        "    print(\"\\n   ‚ùå OVERFITTING SEVERO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar regularizaci√≥n\")\n",
        "    print(\"      - Reducir learning_rate\")\n",
        "    print(\"      - Reducir max_depth\")\n",
        "elif overfit_rmse < 0:\n",
        "    print(\"\\n   ‚ö†Ô∏è OVERFITTING MODERADO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar ligeramente la regularizaci√≥n\")\n",
        "elif overfit_rmse < 200:\n",
        "    print(\"\\n   ‚úÖ BIEN BALANCEADO\")\n",
        "elif overfit_rmse < 500:\n",
        "    print(\"\\n   ‚ö†Ô∏è LIGERO UNDERFITTING\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar max_depth\")\n",
        "    print(\"      - Aumentar learning_rate\")\n",
        "else:\n",
        "    print(\"\\n   ‚ùå UNDERFITTING SEVERO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar complejidad del modelo\")\n",
        "    print(\"      - Reducir regularizaci√≥n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8fbfUKsVVj4",
        "outputId": "b7b4a2f6-52ee-4592-ca09-6024a65d6229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GUARDANDO ARTEFACTOS\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m best_model = models[best_model_name]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_model_name == \u001b[33m'\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mbest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train_transformed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     21\u001b[39m     best_model.fit(X_train_transformed, y_train)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/sube-G17/.venv/lib/python3.12/site-packages/xgboost/core.py:730\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    729\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/sube-G17/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1090\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[39m\n\u001b[32m   1079\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1081\u001b[39m (\n\u001b[32m   1082\u001b[39m     model,\n\u001b[32m   1083\u001b[39m     metric,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[32m   1089\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/sube-G17/.venv/lib/python3.12/site-packages/xgboost/core.py:730\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    729\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/sube-G17/.venv/lib/python3.12/site-packages/xgboost/training.py:181\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/sube-G17/.venv/lib/python3.12/site-packages/xgboost/core.py:2051\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2047\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2050\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2051\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m     )\n\u001b[32m   2055\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2056\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import os, json, joblib, sklearn\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "# IMPORTANTE: Guardar el modelo entrenado, no el modelo sin entrenar\n",
        "# Los modelos ya fueron entrenados en el loop anterior, pero necesitamos guardar el mejor\n",
        "# Re-entrenar el mejor modelo para asegurarnos de que est√© entrenado\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"GUARDANDO ARTEFACTOS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Re-entrenar el mejor modelo con los datos completos\n",
        "best_model = models[best_model_name]\n",
        "if best_model_name == 'XGBoost':\n",
        "    best_model.fit(\n",
        "        X_train_transformed,\n",
        "        y_train,\n",
        "        eval_set=[(X_train_transformed, y_train), (X_valid_transformed, y_valid)],\n",
        "        verbose=False\n",
        "    )\n",
        "else:\n",
        "    best_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üì¶ Guardando fe_pipeline...\")\n",
        "joblib.dump(preprocessor, f\"{ARTIFACT_DIR}/fe_pipeline.joblib\")\n",
        "\n",
        "print(\"üì¶ Guardando final_preprocessor...\")\n",
        "joblib.dump(final_pre, f\"{ARTIFACT_DIR}/preprocessor.joblib\")\n",
        "\n",
        "print(\"üì¶ Guardando modelo...\")\n",
        "joblib.dump(best_model, f\"{ARTIFACT_DIR}/model.joblib\")\n",
        "\n",
        "# Metadata\n",
        "meta = {\n",
        "    \"model_name\": best_model_name,\n",
        "    \"date_col\": \"fecha\",\n",
        "    \"target_col\": \"cantidad\",\n",
        "    \"requires_history\": False,\n",
        "    \"dropped_cols\": [\"fecha\", \"provincia\", \"nombre_feriado\", \"tipo_transporte\", \"tipo_feriado\", \"linea\", \"empresa\"],\n",
        "    \"winsorize_cols\": [\"t_med\", \"precip\", \"tmax\", \"tmin\"],\n",
        "    \"sklearn_version\": sklearn.__version__,\n",
        "    \"metrics\": {\n",
        "        \"valid_rmse\": best_rmse,\n",
        "        \"valid_r2\": best_r2,\n",
        "        \"training_time_seconds\": training_times[best_model_name]\n",
        "    }\n",
        "}\n",
        "json.dump(meta, open(f\"{ARTIFACT_DIR}/metadata.json\", \"w\"), indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Artefactos guardados exitosamente en '{ARTIFACT_DIR}/'\")\n",
        "print(f\"\\nüìã Archivos generados:\")\n",
        "print(f\"   - fe_pipeline.joblib (feature engineering)\")\n",
        "print(f\"   - preprocessor.joblib (encoding + scaling)\")\n",
        "print(f\"   - model.joblib ({best_model_name})\")\n",
        "print(f\"   - metadata.json (configuraci√≥n)\")\n",
        "\n",
        "# Verificaci√≥n: cargar y probar\n",
        "print(f\"\\nüîç Verificando carga de artefactos...\")\n",
        "fe_loaded = joblib.load(f\"{ARTIFACT_DIR}/fe_pipeline.joblib\")\n",
        "prep_loaded = joblib.load(f\"{ARTIFACT_DIR}/preprocessor.joblib\")\n",
        "model_loaded = joblib.load(f\"{ARTIFACT_DIR}/model.joblib\")\n",
        "meta_loaded = json.load(open(f\"{ARTIFACT_DIR}/metadata.json\"))\n",
        "\n",
        "print(f\"   ‚úÖ Feature engineering pipeline cargado\")\n",
        "print(f\"   ‚úÖ Preprocessor cargado\")\n",
        "print(f\"   ‚úÖ Modelo {meta_loaded['model_name']} cargado\")\n",
        "print(f\"   ‚úÖ Metadata cargada\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"‚úÖ EXPORTACI√ìN COMPLETADA\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgDK7svup7Eg"
      },
      "source": [
        "## 4. Ajuste de hiperpar√°metros\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFbj6mbku1M8",
        "outputId": "3ebb9e70-ff9c-4f26-d955-6a300375ca9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Espacio de b√∫squeda optimizado:\n",
            "   Total de combinaciones posibles: 8748\n",
            "   - n_estimators: 3 opciones\n",
            "   - max_depth: 4 opciones\n",
            "   - min_samples_split: 3 opciones\n",
            "   - min_samples_leaf: 3 opciones\n",
            "   - max_features: 3 opciones\n",
            "   - bootstrap: 1 opciones\n",
            "   - max_samples: 3 opciones\n",
            "   - max_leaf_nodes: 3 opciones\n",
            "   - min_impurity_decrease: 3 opciones\n"
          ]
        }
      ],
      "source": [
        "param_distributions = {\n",
        "    # N√∫mero de √°rboles: menos opciones, valores m√°s razonables\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    \n",
        "    # Profundidad: enfocado en valores que eviten overfitting\n",
        "    'max_depth': [8, 10, 12, 15],\n",
        "    \n",
        "    # Muestras por split: valores que balanceen bias-variance\n",
        "    'min_samples_split': [5, 10, 15],\n",
        "    \n",
        "    # Muestras por hoja: control de overfitting\n",
        "    'min_samples_leaf': [2, 4, 8],\n",
        "    \n",
        "    # Features por split: opciones estrat√©gicas\n",
        "    'max_features': ['sqrt', 0.5, 0.7],\n",
        "    \n",
        "    # Bootstrap: siempre True es m√°s r√°pido y generalmente mejor\n",
        "    'bootstrap': [True],\n",
        "    \n",
        "    # Porcentaje de muestras: enfocado en valores que mejoren generalizaci√≥n\n",
        "    'max_samples': [0.7, 0.8, 0.9],\n",
        "    \n",
        "    # NUEVO: Controlar complejidad de √°rboles individuales\n",
        "    'max_leaf_nodes': [None, 50, 100],\n",
        "    \n",
        "    # NUEVO: Regularizaci√≥n adicional\n",
        "    'min_impurity_decrease': [0.0, 0.0001, 0.001]\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Espacio de b√∫squeda optimizado:\")\n",
        "print(f\"   Total de combinaciones posibles: {np.prod([len(v) for v in param_distributions.values()])}\")\n",
        "for param, values in param_distributions.items():\n",
        "    print(f\"   - {param}: {len(values)} opciones\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYAZWA2zu9F1",
        "outputId": "2c4c08dc-f1ee-4b01-cd07-d011bbce46af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Configurando b√∫squeda aleatoria optimizada...\n",
            "‚úì Configuraci√≥n completada\n",
            "   - Iteraciones: 20 (balance velocidad/calidad)\n",
            "   - Cross-validation: 3 folds\n",
            "   - Total de fits: 60 = 60\n",
            "   - Tiempo estimado: 5-10 minutos\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîç Configurando b√∫squeda aleatoria optimizada...\")\n",
        "\n",
        "# CLAVE: Reducir n_iter pero mantener cv m√°s peque√±o para velocidad\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        warm_start=False,  # No usar warm_start en b√∫squeda\n",
        "        oob_score=True     # Usar out-of-bag score para evaluaci√≥n adicional\n",
        "    ),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,              # REDUCIDO: 20 iteraciones en lugar de 30\n",
        "    cv=3,                   # 3-fold es buen balance velocidad/precisi√≥n\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1,              # Paralelizar todo\n",
        "    random_state=42,\n",
        "    verbose=1,              # Menos verbose para menos overhead\n",
        "    return_train_score=True,\n",
        "    refit=True              # Refit autom√°tico con mejores par√°metros\n",
        ")\n",
        "\n",
        "print(\"‚úì Configuraci√≥n completada\")\n",
        "print(f\"   - Iteraciones: 20 (balance velocidad/calidad)\")\n",
        "print(f\"   - Cross-validation: 3 folds\")\n",
        "print(f\"   - Total de fits: {20 * 3} = 60\")\n",
        "print(f\"   - Tiempo estimado: 5-10 minutos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEJHwrsUV4Ny",
        "outputId": "b70d69b4-a5cc-43b7-892e-90aa3ff9527a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Iniciando b√∫squeda de hiperpar√°metros...\n",
            "   (Esto tomar√° aproximadamente 5-10 minutos...)\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ B√∫squeda completada en 6.52 minutos (390.91 segundos)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ Iniciando b√∫squeda de hiperpar√°metros...\")\n",
        "print(\"   (Esto tomar√° aproximadamente 5-10 minutos...)\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "random_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "search_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ B√∫squeda completada en {search_time/60:.2f} minutos ({search_time:.2f} segundos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d_4M1l_V-EK",
        "outputId": "efe77142-2641-476d-859e-bd30b3018a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTADOS DE LA B√öSQUEDA\n",
            "================================================================================\n",
            "\n",
            "üèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\n",
            "   n_estimators: 150\n",
            "   min_samples_split: 15\n",
            "   min_samples_leaf: 2\n",
            "   min_impurity_decrease: 0.0\n",
            "   max_samples: 0.8\n",
            "   max_leaf_nodes: None\n",
            "   max_features: 0.5\n",
            "   max_depth: 12\n",
            "   bootstrap: True\n",
            "\n",
            "üìä Mejor RMSE en Cross-Validation: 3125.38\n",
            "üìä OOB Score del mejor modelo: 0.9803\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# AN√ÅLISIS DE RESULTADOS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS DE LA B√öSQUEDA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\")\n",
        "for param, value in random_search.best_params_.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "# Score del mejor modelo\n",
        "best_cv_rmse = -random_search.best_score_\n",
        "print(f\"\\nüìä Mejor RMSE en Cross-Validation: {best_cv_rmse:.2f}\")\n",
        "\n",
        "# Informaci√≥n del mejor estimador\n",
        "best_model = random_search.best_estimator_\n",
        "if hasattr(best_model, 'oob_score_'):\n",
        "    oob_rmse = np.sqrt(-best_model.oob_score_) if best_model.oob_score_ < 0 else np.sqrt(1 - best_model.oob_score_)\n",
        "    print(f\"üìä OOB Score del mejor modelo: {best_model.oob_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPo5o_0WFEG",
        "outputId": "e1433f85-d856-42d9-bdba-41bab9bdbbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ENTRENAMIENTO DEL MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "\n",
            "üîÑ Entrenando Random Forest con hiperpar√°metros optimizados...\n",
            "‚úì Modelo entrenado en 9.93 segundos\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENTRENAMIENTO DEL MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîÑ Entrenando Random Forest con hiperpar√°metros optimizados...\")\n",
        "\n",
        "final_rf_model = RandomForestRegressor(\n",
        "    **random_search.best_params_,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start_train = time.time()\n",
        "final_rf_model.fit(X_train_transformed, y_train)\n",
        "train_time = time.time() - start_train\n",
        "\n",
        "print(f\"‚úì Modelo entrenado en {train_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKjBp6ZXWXPj",
        "outputId": "28667f3c-0eb3-4dc1-ad78-3a4bf0d35800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Generando predicciones...\n",
            "\n",
            "================================================================================\n",
            "M√âTRICAS DEL MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "           Dataset        RMSE         MAE       R¬≤    MAPE (%)\n",
            "Train (Optimizado) 2432.196121 1132.818424 0.985591  132.584015\n",
            "Valid (Optimizado) 3776.322544 1564.476856 0.960879 1211.014633\n",
            " Test (Optimizado) 2758.393517 1195.714962 0.980744   46.913720\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä Generando predicciones...\")\n",
        "\n",
        "y_train_pred_opt = final_rf_model.predict(X_train_transformed)\n",
        "y_valid_pred_opt = final_rf_model.predict(X_valid_transformed)\n",
        "y_test_pred_opt = final_rf_model.predict(X_test_transformed)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "metrics_train_opt = calculate_metrics(y_train, y_train_pred_opt, 'Train (Optimizado)')\n",
        "metrics_valid_opt = calculate_metrics(y_valid, y_valid_pred_opt, 'Valid (Optimizado)')\n",
        "metrics_test_opt = calculate_metrics(y_test, y_test_pred_opt, 'Test (Optimizado)')\n",
        "\n",
        "results_opt_df = pd.DataFrame([metrics_train_opt, metrics_valid_opt, metrics_test_opt])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"M√âTRICAS DEL MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "print(results_opt_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCDS-crGWZKJ",
        "outputId": "d31e58d2-fa14-46d8-8010-f545146b3add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARACI√ìN: MODELO ANTERIOR vs MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "\n",
            "Conjunto  RMSE Anterior  RMSE Optimizado  R¬≤ Anterior  R¬≤ Optimizado  Mejora RMSE  Mejora R¬≤\n",
            "   Train    1950.085575      2432.196121     0.990737       0.985591  -482.110546  -0.005146\n",
            "   Valid    3666.564332      3776.322544     0.963120       0.960879  -109.758212  -0.002241\n",
            "    Test    2681.070402      2758.393517     0.981808       0.980744   -77.323115  -0.001064\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN: MODELO ANTERIOR vs MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_data = {\n",
        "    'Conjunto': ['Train', 'Valid', 'Test'],\n",
        "    'RMSE Anterior': [\n",
        "        best_train_metrics['RMSE'],\n",
        "        modelo_anterior_valid_rmse,\n",
        "        modelo_anterior_test_rmse\n",
        "    ],\n",
        "    'RMSE Optimizado': [\n",
        "        metrics_train_opt['RMSE'],\n",
        "        metrics_valid_opt['RMSE'],\n",
        "        metrics_test_opt['RMSE']\n",
        "    ],\n",
        "    'R¬≤ Anterior': [\n",
        "        best_train_metrics['R¬≤'],\n",
        "        modelo_anterior_valid_r2,\n",
        "        modelo_anterior_test_r2\n",
        "    ],\n",
        "    'R¬≤ Optimizado': [\n",
        "        metrics_train_opt['R¬≤'],\n",
        "        metrics_valid_opt['R¬≤'],\n",
        "        metrics_test_opt['R¬≤']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Mejora RMSE'] = comparison_df['RMSE Anterior'] - comparison_df['RMSE Optimizado']\n",
        "comparison_df['Mejora R¬≤'] = comparison_df['R¬≤ Optimizado'] - comparison_df['R¬≤ Anterior']\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXDb8TiLWkuq",
        "outputId": "1be7eb9c-a89f-4dfd-b0a9-0782b86ecca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "AN√ÅLISIS DE MEJORAS\n",
            "================================================================================\n",
            "\n",
            "üìà MEJORAS EN VALIDATION SET:\n",
            "   Mejora RMSE: -74.78 (-0.41%)\n",
            "   Mejora R¬≤: -0.0076\n",
            "\n",
            "   ‚ö†Ô∏è El modelo anterior era mejor o similar\n",
            "   üí° Considerar:\n",
            "      - El modelo base ya estaba bien ajustado\n",
            "      - Posible overfitting en la b√∫squeda de hiperpar√°metros\n",
            "\n",
            "üîç DIAGN√ìSTICO DE OVERFITTING:\n",
            "   Diferencia Train-Valid RMSE: +410.06\n",
            "   ‚ö†Ô∏è UNDERFITTING LEVE: El modelo podr√≠a ser m√°s complejo\n",
            "\n",
            "üìä CONSISTENCIA VALID-TEST:\n",
            "   Diferencia RMSE: 822.54\n",
            "   ‚ö†Ô∏è Hay diferencia significativa entre Valid y Test\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISIS DE MEJORAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mejora_rmse_valid = modelo_anterior_valid_rmse - metrics_valid_opt['RMSE']\n",
        "mejora_r2_valid = metrics_valid_opt['R¬≤'] - modelo_anterior_valid_r2\n",
        "mejora_porcentual = (mejora_rmse_valid / modelo_anterior_valid_rmse) * 100\n",
        "\n",
        "print(f\"\\nüìà MEJORAS EN VALIDATION SET:\")\n",
        "print(f\"   Mejora RMSE: {mejora_rmse_valid:+.2f} ({mejora_porcentual:+.2f}%)\")\n",
        "print(f\"   Mejora R¬≤: {mejora_r2_valid:+.4f}\")\n",
        "\n",
        "if mejora_rmse_valid > 50:\n",
        "    print(\"\\n   üèÜ EXCELENTE: Mejora significativa con la optimizaci√≥n\")\n",
        "elif mejora_rmse_valid > 10:\n",
        "    print(\"\\n   ‚úÖ BUENO: Mejora notable con la optimizaci√≥n\")\n",
        "elif mejora_rmse_valid > 0:\n",
        "    print(\"\\n   ‚úì POSITIVO: Ligera mejora con la optimizaci√≥n\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è El modelo anterior era mejor o similar\")\n",
        "    print(\"   üí° Considerar:\")\n",
        "    print(\"      - El modelo base ya estaba bien ajustado\")\n",
        "    print(\"      - Posible overfitting en la b√∫squeda de hiperpar√°metros\")\n",
        "\n",
        "# An√°lisis de overfitting\n",
        "overfit_rmse_opt = metrics_train_opt['RMSE'] - metrics_valid_opt['RMSE']\n",
        "\n",
        "print(f\"\\nüîç DIAGN√ìSTICO DE OVERFITTING:\")\n",
        "print(f\"   Diferencia Train-Valid RMSE: {overfit_rmse_opt:+.2f}\")\n",
        "\n",
        "if abs(overfit_rmse_opt) < 200:\n",
        "    print(\"   ‚úÖ BIEN BALANCEADO: Modelo optimizado bien generalizado\")\n",
        "elif overfit_rmse_opt < 0:\n",
        "    print(\"   ‚ö†Ô∏è OVERFITTING DETECTADO: El modelo memoriza datos de train\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è UNDERFITTING LEVE: El modelo podr√≠a ser m√°s complejo\")\n",
        "\n",
        "# Consistencia Valid-Test\n",
        "valid_test_diff = abs(metrics_valid_opt['RMSE'] - metrics_test_opt['RMSE'])\n",
        "print(f\"\\nüìä CONSISTENCIA VALID-TEST:\")\n",
        "print(f\"   Diferencia RMSE: {valid_test_diff:.2f}\")\n",
        "\n",
        "if valid_test_diff < 300:\n",
        "    print(\"   ‚úÖ EXCELENTE: Performance consistente\")\n",
        "elif valid_test_diff < 500:\n",
        "    print(\"   ‚úÖ BUENO: Performance aceptable\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Hay diferencia significativa entre Valid y Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tIo-WHr9rb2"
      },
      "source": [
        "## 6. Predicci√≥n final (ejemplo de una fecha futura)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uZUu_nhovOfV",
        "outputId": "b24d888d-caa4-4090-d634-69ed9a94119c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PREDICCI√ìN FINAL - EJEMPLO PR√ÅCTICO\n",
            "================================================================================\n",
            "\n",
            "üìã EJEMPLO 1: PREDICCI√ìN CON DATOS REALES DEL TEST SET\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìÖ INFORMACI√ìN DE LA PREDICCI√ìN:\n",
            "   Fecha: 2024-11-28 (Thursday)\n",
            "   L√≠nea: BSAS_LINEA_506\n",
            "   Municipio: florencio varela\n",
            "   Empresa: TRANSPORTES SAN JUAN BAUTISTA S.A.\n",
            "\n",
            "üå§Ô∏è CONDICIONES CLIM√ÅTICAS:\n",
            "   Temperatura m√°xima: 27.3¬∞C\n",
            "   Temperatura m√≠nima: 17.7¬∞C\n",
            "   Temperatura media: 22.5¬∞C\n",
            "   Precipitaci√≥n: 1.0 mm\n",
            "   Viento: 22.9 km/h\n",
            "\n",
            "üìä CONTEXTO TEMPORAL:\n",
            "   D√≠a de la semana: Jueves\n",
            "   Es fin de semana: No\n",
            "   Mes: 11\n",
            "   Es feriado: No\n",
            "\n",
            "üìä PERFILES HIST√ìRICOS:\n",
            "   (Los perfiles hist√≥ricos se calculan autom√°ticamente durante la predicci√≥n)\n",
            "   El modelo utiliza estad√≠sticas hist√≥ricas de pasajeros por:\n",
            "   - L√≠nea + Municipio + D√≠a de la semana\n",
            "   - L√≠nea + Municipio\n",
            "   - L√≠nea + D√≠a de la semana\n",
            "   - Municipio + D√≠a de la semana\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'std'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'std'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3347439815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Preprocesar y predecir (usar ambos pipelines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msample_data_fe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0msample_data_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_fe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_rf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3631316190.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Features derivadas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volatility'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Coeficiente de variaci√≥n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_demand'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lm_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Demanda relativa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'std'"
          ]
        }
      ],
      "source": [
        "## 6. Predicci√≥n Final - Ejemplo Pr√°ctico\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICCI√ìN FINAL - EJEMPLO PR√ÅCTICO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.1 Ejemplo con datos reales del Test Set\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìã EJEMPLO 1: PREDICCI√ìN CON DATOS REALES DEL TEST SET\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Tomar una muestra aleatoria del test set\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "sample_data_raw = X_test.iloc[sample_idx:sample_idx+1].copy()\n",
        "\n",
        "# Obtener fecha correspondiente del dataframe del test set\n",
        "sample_date_info = df_test.iloc[sample_idx]\n",
        "\n",
        "print(\"\\nüìÖ INFORMACI√ìN DE LA PREDICCI√ìN:\")\n",
        "print(f\"   Fecha: {sample_date_info['fecha'].strftime('%Y-%m-%d (%A)')}\")\n",
        "print(f\"   L√≠nea: {sample_data_raw['linea'].iloc[0]}\")\n",
        "print(f\"   Municipio: {sample_data_raw['municipio'].iloc[0]}\")\n",
        "print(f\"   Empresa: {sample_data_raw['empresa'].iloc[0]}\")\n",
        "\n",
        "print(f\"\\nüå§Ô∏è CONDICIONES CLIM√ÅTICAS:\")\n",
        "print(f\"   Temperatura m√°xima: {sample_data_raw['tmax'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Temperatura m√≠nima: {sample_data_raw['tmin'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Temperatura media: {sample_data_raw['t_med'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Precipitaci√≥n: {sample_data_raw['precip'].iloc[0]:.1f} mm\")\n",
        "print(f\"   Viento: {sample_data_raw['viento'].iloc[0]:.1f} km/h\")\n",
        "\n",
        "print(f\"\\nüìä CONTEXTO TEMPORAL:\")\n",
        "dia_nombres = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']\n",
        "dia_semana_num = int(sample_data_raw['dia_semana'].iloc[0])\n",
        "print(f\"   D√≠a de la semana: {dia_nombres[dia_semana_num]}\")\n",
        "print(f\"   Es fin de semana: {'S√≠' if sample_data_raw['is_weekend'].iloc[0] else 'No'}\")\n",
        "print(f\"   Mes: {sample_data_raw['mes'].iloc[0]}\")\n",
        "print(f\"   Es feriado: {'S√≠' if sample_data_raw['is_feriado'].iloc[0] else 'No'}\")\n",
        "\n",
        "print(f\"\\nüìä PERFILES HIST√ìRICOS:\")\n",
        "print(\"   (Los perfiles hist√≥ricos se calculan autom√°ticamente durante la predicci√≥n)\")\n",
        "print(\"   El modelo utiliza estad√≠sticas hist√≥ricas de pasajeros por:\")\n",
        "print(\"   - L√≠nea + Municipio + D√≠a de la semana\")\n",
        "print(\"   - L√≠nea + Municipio\")\n",
        "print(\"   - L√≠nea + D√≠a de la semana\")\n",
        "print(\"   - Municipio + D√≠a de la semana\")\n",
        "\n",
        "# Preprocesar y predecir (usar ambos pipelines)\n",
        "sample_data_fe = preprocessor.transform(sample_data_raw)\n",
        "sample_data_transformed = final_pre.transform(sample_data_fe)\n",
        "prediction = final_rf_model.predict(sample_data_transformed)[0]\n",
        "actual = y_test.iloc[sample_idx]\n",
        "\n",
        "print(f\"\\nüéØ RESULTADO DE LA PREDICCI√ìN:\")\n",
        "print(f\"   Predicci√≥n del modelo: {prediction:.0f} pasajeros\")\n",
        "print(f\"   Valor real observado: {actual:.0f} pasajeros\")\n",
        "print(f\"   Error absoluto: {abs(prediction - actual):.0f} pasajeros\")\n",
        "\n",
        "# Evaluaci√≥n cualitativa del error\n",
        "error_pct = abs(prediction - actual) / actual * 100\n",
        "if error_pct < 5:\n",
        "    print(\"   ‚úÖ Excelente: Error menor al 5%\")\n",
        "elif error_pct < 10:\n",
        "    print(\"   ‚úÖ Muy bueno: Error menor al 10%\")\n",
        "elif error_pct < 20:\n",
        "    print(\"   ‚úì Aceptable: Error menor al 20%\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Alto: Error mayor al 20%\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.2 Funci√≥n para predicciones futuras\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FUNCI√ìN DE PREDICCI√ìN FUTURA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_future_passengers(linea, municipio, empresa, fecha, tmax, tmin, precip, viento,\n",
        "                              lag_1=None, lag_7=None, lag_28=None, is_feriado=0, tipo_feriado=None):\n",
        "    \"\"\"\n",
        "    Predice la cantidad de pasajeros para una fecha futura.\n",
        "\n",
        "    Par√°metros:\n",
        "    -----------\n",
        "    linea : str\n",
        "        C√≥digo de la l√≠nea de colectivo (ej: 'BSAS_LINEA_740')\n",
        "    municipio : str\n",
        "        Nombre del municipio (ej: 'san miguel')\n",
        "    empresa : str\n",
        "        Nombre de la empresa operadora\n",
        "    fecha : str\n",
        "        Fecha en formato 'YYYY-MM-DD'\n",
        "    tmax : float\n",
        "        Temperatura m√°xima esperada (¬∞C)\n",
        "    tmin : float\n",
        "        Temperatura m√≠nima esperada (¬∞C)\n",
        "    precip : float\n",
        "        Precipitaci√≥n esperada (mm)\n",
        "    viento : float\n",
        "        Velocidad del viento esperada (km/h)\n",
        "    lag_1 : float, optional\n",
        "        Pasajeros del d√≠a anterior (si no se provee, usa promedio hist√≥rico)\n",
        "    lag_7 : float, optional\n",
        "        Pasajeros hace 7 d√≠as (si no se provee, usa promedio hist√≥rico)\n",
        "    lag_28 : float, optional\n",
        "        Pasajeros hace 28 d√≠as (si no se provee, usa promedio hist√≥rico)\n",
        "    is_feriado : int, optional\n",
        "        1 si es feriado, 0 si no lo es (default: 0)\n",
        "    tipo_feriado : str, optional\n",
        "        Tipo de feriado si corresponde\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "    float\n",
        "        Cantidad estimada de pasajeros\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # ESTO SE EVITA CON EL PIPELINE DE LA PARTE DEL PRINCIPIO!!!\n",
        "    # Convertir fecha a datetime\n",
        "    fecha_dt = pd.to_datetime(fecha)\n",
        "\n",
        "    # Calcular features temporales\n",
        "    dia_semana = fecha_dt.dayofweek\n",
        "    mes = fecha_dt.month\n",
        "    is_weekend = 1 if dia_semana >= 5 else 0\n",
        "\n",
        "    # Features c√≠clicas\n",
        "    mes_sin = np.sin(2 * np.pi * mes / 12)\n",
        "    mes_cos = np.cos(2 * np.pi * mes / 12)\n",
        "    dia_sin = np.sin(2 * np.pi * dia_semana / 7)\n",
        "    dia_cos = np.cos(2 * np.pi * dia_semana / 7)\n",
        "\n",
        "    # Features de temperatura\n",
        "    t_med = (tmax + tmin) / 2\n",
        "    t_amp = tmax - tmin\n",
        "\n",
        "    # Valores por defecto para lags si no se proveen\n",
        "    # Usar promedio general del training set\n",
        "    if lag_1 is None:\n",
        "        lag_1 = y_train.mean()\n",
        "    if lag_7 is None:\n",
        "        lag_7 = y_train.mean()\n",
        "    if lag_28 is None:\n",
        "        lag_28 = y_train.mean()\n",
        "\n",
        "    roll_7 = lag_1  # Aproximaci√≥n\n",
        "    roll_28 = lag_7  # Aproximaci√≥n\n",
        "\n",
        "    # Crear DataFrame con todos los features\n",
        "    future_data = pd.DataFrame({\n",
        "        'empresa': [empresa],\n",
        "        'linea': [linea],\n",
        "        'municipio': [municipio],\n",
        "        'tmax': [tmax],\n",
        "        'tmin': [tmin],\n",
        "        'precip': [precip],\n",
        "        'viento': [viento],\n",
        "        'is_feriado': [is_feriado],\n",
        "        'tipo_feriado': [tipo_feriado],\n",
        "        'dia_semana': [dia_semana],\n",
        "        'mes': [mes],\n",
        "        'is_weekend': [is_weekend],\n",
        "        'mes_sin': [mes_sin],\n",
        "        'mes_cos': [mes_cos],\n",
        "        'dia_sin': [dia_sin],\n",
        "        'dia_cos': [dia_cos],\n",
        "        't_med': [t_med],\n",
        "        't_amp': [t_amp],\n",
        "        'lag_1': [lag_1],\n",
        "        'lag_7': [lag_7],\n",
        "        'lag_28': [lag_28],\n",
        "        'roll_7': [roll_7],\n",
        "        'roll_28': [roll_28],\n",
        "        'has_lag_1': [0],\n",
        "        'has_lag_7': [0],\n",
        "        'has_lag_28': [0]\n",
        "    })\n",
        "\n",
        "      # Aplicar feature engineering primero\n",
        "    future_data_fe = preprocessor.transform(future_data)\n",
        "      # Luego aplicar preprocesamiento final\n",
        "    future_data_transformed = final_pre.transform(future_data_fe)\n",
        "    prediction = final_rf_model.predict(future_data_transformed)[0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "print(\"\\n‚úÖ Funci√≥n 'predict_future_passengers' creada\")\n",
        "print(\"\\nüìù Par√°metros de la funci√≥n:\")\n",
        "print(\"   - linea: C√≥digo de l√≠nea (obligatorio)\")\n",
        "print(\"   - municipio: Municipio (obligatorio)\")\n",
        "print(\"   - empresa: Empresa operadora (obligatorio)\")\n",
        "print(\"   - fecha: Fecha YYYY-MM-DD (obligatorio)\")\n",
        "print(\"   - tmax, tmin: Temperaturas en ¬∞C (obligatorio)\")\n",
        "print(\"   - precip: Precipitaci√≥n en mm (obligatorio)\")\n",
        "print(\"   - viento: Velocidad en km/h (obligatorio)\")\n",
        "print(\"   - lag_1, lag_7, lag_28: Datos hist√≥ricos (opcional)\")\n",
        "print(\"   - is_feriado: 0 o 1 (opcional, default: 0)\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.3 Ejemplos de uso de la funci√≥n\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EJEMPLOS DE PREDICCIONES FUTURAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ejemplo 1: D√≠a laboral normal\n",
        "print(\"\\nüìù EJEMPLO 1: D√çA LABORAL NORMAL (Mi√©rcoles)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo1 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-15\",  # Mi√©rcoles\n",
        "    tmax=28.0,\n",
        "    tmin=20.0,\n",
        "    precip=0.0,\n",
        "    viento=15.0,\n",
        "    lag_1=15000,  # Ayer hubo 15,000 pasajeros\n",
        "    lag_7=14500,  # Hace una semana hubo 14,500\n",
        "    lag_28=14000, # Hace un mes hubo 14,000\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-15 (Mi√©rcoles)\")\n",
        "print(f\"   Condiciones: Buen clima, sin lluvia\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo1:.0f} pasajeros\")\n",
        "\n",
        "# Ejemplo 2: Fin de semana\n",
        "print(\"\\nüìù EJEMPLO 2: FIN DE SEMANA (Domingo)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo2 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-19\",  # Domingo\n",
        "    tmax=30.0,\n",
        "    tmin=22.0,\n",
        "    precip=0.0,\n",
        "    viento=10.0,\n",
        "    lag_1=8000,   # Los domingos hay menos pasajeros\n",
        "    lag_7=14500,\n",
        "    lag_28=14000,\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-19 (Domingo)\")\n",
        "print(f\"   Condiciones: D√≠a soleado\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo2:.0f} pasajeros\")\n",
        "print(f\"   Nota: Menor que d√≠a laboral (esperado para fin de semana)\")\n",
        "\n",
        "# Ejemplo 3: D√≠a con lluvia\n",
        "print(\"\\nüìù EJEMPLO 3: D√çA LLUVIOSO (Lunes)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo3 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-20\",  # Lunes\n",
        "    tmax=22.0,\n",
        "    tmin=18.0,\n",
        "    precip=25.0,  # Lluvia intensa\n",
        "    viento=30.0,  # Viento fuerte\n",
        "    lag_1=8000,   # Domingo (menos pasajeros)\n",
        "    lag_7=14500,\n",
        "    lag_28=14000,\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-20 (Lunes)\")\n",
        "print(f\"   Condiciones: Lluvia intensa, viento fuerte\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo3:.0f} pasajeros\")\n",
        "print(f\"   Nota: El clima puede afectar la demanda\")\n",
        "\n",
        "# Ejemplo 4: Feriado\n",
        "print(\"\\nüìù EJEMPLO 4: D√çA FERIADO\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo4 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-05-01\",  # 1 de Mayo (D√≠a del Trabajador)\n",
        "    tmax=25.0,\n",
        "    tmin=18.0,\n",
        "    precip=0.0,\n",
        "    viento=12.0,\n",
        "    lag_1=12000,\n",
        "    lag_7=14000,\n",
        "    lag_28=14500,\n",
        "    is_feriado=1,  # Es feriado\n",
        "    tipo_feriado=\"inamovible\"\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-05-01 (D√≠a del Trabajador)\")\n",
        "print(f\"   Condiciones: Feriado nacional\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo4:.0f} pasajeros\")\n",
        "print(f\"   Nota: Los feriados t√≠picamente tienen menos demanda\")\n",
        "\n",
        "# Ejemplo 5: Sin datos hist√≥ricos (usando promedios)\n",
        "print(\"\\nüìù EJEMPLO 5: SIN DATOS HIST√ìRICOS (Nueva ruta o predicci√≥n lejana)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo5 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-06-15\",  # Fecha lejana\n",
        "    tmax=20.0,\n",
        "    tmin=12.0,\n",
        "    precip=0.0,\n",
        "    viento=18.0,\n",
        "    # No se proveen lags, usa promedios autom√°ticamente\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-06-15\")\n",
        "print(f\"   Condiciones: Sin datos hist√≥ricos recientes\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo5:.0f} pasajeros\")\n",
        "print(f\"   Nota: Basado en promedios hist√≥ricos generales\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.4 Resumen y recomendaciones\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN Y RECOMENDACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° C√ìMO USAR LA FUNCI√ìN DE PREDICCI√ìN:\")\n",
        "print(\"   1. Preparar datos de entrada (l√≠nea, municipio, empresa)\")\n",
        "print(\"   2. Obtener pron√≥stico clim√°tico para la fecha\")\n",
        "print(\"   3. Si es posible, incluir datos hist√≥ricos recientes (lags)\")\n",
        "print(\"   4. Indicar si la fecha es feriado\")\n",
        "print(\"   5. La funci√≥n retorna la cantidad estimada de pasajeros\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è CONSIDERACIONES IMPORTANTES:\")\n",
        "print(\"   - Las predicciones son m√°s precisas con datos hist√≥ricos recientes\")\n",
        "print(\"   - El clima puede afectar significativamente la demanda\")\n",
        "print(\"   - Los fines de semana y feriados tienen patrones diferentes\")\n",
        "print(\"   - Para rutas nuevas, la precisi√≥n ser√° menor\")\n",
        "\n",
        "print(\"\\n‚úÖ MODELO LISTO PARA PRODUCCI√ìN\")\n",
        "print(\"   El modelo ha sido entrenado, validado y est√° listo para:\")\n",
        "print(\"   - Predicciones en tiempo real\")\n",
        "print(\"   - Planificaci√≥n de servicios\")\n",
        "print(\"   - Optimizaci√≥n de frecuencias\")\n",
        "print(\"   - An√°lisis de demanda futura\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHHnqxH9xMT"
      },
      "source": [
        "## 7. Conclusiones del trabajo"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
