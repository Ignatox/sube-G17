{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU1Cu0-pz1eW"
      },
      "source": [
        "# Tercera entrega - Trabajo Integrador - **Grupo 17 5k10**\n",
        "##  **Integrantes:** Veggiani Franco, Diaz Juan Ignacio\n",
        "\n",
        "### Enunciado:\n",
        "En esta entrega se espera:\n",
        "1. Que cada grupo defina claramente el objetivo predictivo del proyecto, identificando con precisi√≥n la variable objetivo y el tipo de problema (clasificaci√≥n, regresi√≥n, etc.).\n",
        "2. El an√°lisis debe incluir la estrategia de partici√≥n del conjunto de datos (entrenamiento/test o validaci√≥n cruzada), y la construcci√≥n de un pipeline completo de Scikit-learn que integre todas las etapas necesarias: preprocesamiento, modelado y evaluaci√≥n.\n",
        "3. Deber√°n comparar al menos tres modelos distintos y justificar cu√°l se ajusta mejor al problema.\n",
        "4. Luego, deber√°n realizar un proceso de b√∫squeda y ajuste de hiperpar√°metros sobre el modelo elegido, utilizando t√©cnicas como GridSearchCV o RandomizedSearchCV.\n",
        "5. Se espera una evaluaci√≥n rigurosa con m√©tricas adecuadas (accuracy, F1, RMSE, etc., seg√∫n el caso), as√≠ como un diagn√≥stico de overfitting o underfitting y posibles acciones para mitigarlos.\n",
        "\n",
        "La entrega debe presentarse como un Jupyter Notebook claro, bien estructurado y reproducible, y los integrantes del grupo deben poder explicar oralmente las decisiones tomadas durante todo el proceso de modelado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULmiFYOgC9Zp"
      },
      "source": [
        "## 0. Definici√≥n del pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pzQpHhMDDfhu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pG9D_Qn2dGyI"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ALTERNATIVA A LAGS: FEATURES AGREGADAS HIST√ìRICAS\n",
        "# ============================================================================\n",
        "# En lugar de usar lag_1, lag_7, etc., usamos estad√≠sticas agregadas\n",
        "# por grupo que se calculan UNA VEZ y se guardan como \"lookup table\"\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class HistoricalProfileEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica el \"perfil hist√≥rico\" de cada l√≠nea-municipio-d√≠a_semana.\n",
        "\n",
        "    En ENTRENAMIENTO:\n",
        "    - Calcula estad√≠sticas agregadas por grupo\n",
        "\n",
        "    En PREDICCI√ìN:\n",
        "    - Usa las estad√≠sticas pre-calculadas (NO necesita historial reciente)\n",
        "\n",
        "    Ejemplo:\n",
        "    --------\n",
        "    Para predecir la l√≠nea 740 en San Miguel un Lunes:\n",
        "    - NO usa cu√°ntos pasajeros hubo ayer (lag_1) ‚úó\n",
        "    - S√ç usa \"los lunes en esta l√≠nea suelen haber X pasajeros\" ‚úì\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, group_cols=['linea', 'municipio', 'dia_semana']):\n",
        "        self.group_cols = group_cols\n",
        "        self.profiles = {}\n",
        "        self.global_stats = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula perfiles hist√≥ricos desde los datos de entrenamiento\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüìä Calculando perfiles hist√≥ricos...\")\n",
        "\n",
        "        # 1. Estad√≠sticas por grupo (l√≠nea + municipio + d√≠a de semana)\n",
        "        group_stats = df.groupby(self.group_cols)['cantidad'].agg([\n",
        "            'mean',    # Promedio hist√≥rico\n",
        "            'std',     # Variabilidad\n",
        "            'median',  # Mediana (robusto a outliers)\n",
        "            'min',     # M√≠nimo hist√≥rico\n",
        "            'max',     # M√°ximo hist√≥rico\n",
        "            'count'    # Cantidad de observaciones\n",
        "        ]).reset_index()\n",
        "\n",
        "        self.profiles['main'] = group_stats\n",
        "\n",
        "        # 2. Estad√≠sticas por l√≠nea + municipio (sin d√≠a de semana)\n",
        "        line_muni_stats = df.groupby(['linea', 'municipio'])['cantidad'].agg([\n",
        "            'mean', 'std', 'median'\n",
        "        ]).reset_index()\n",
        "        line_muni_stats.columns = ['linea', 'municipio', 'lm_mean', 'lm_std', 'lm_median']\n",
        "        self.profiles['line_muni'] = line_muni_stats\n",
        "\n",
        "        # 3. Estad√≠sticas por l√≠nea + d√≠a de semana\n",
        "        line_day_stats = df.groupby(['linea', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        line_day_stats.columns = ['linea', 'dia_semana', 'ld_mean', 'ld_std']\n",
        "        self.profiles['line_day'] = line_day_stats\n",
        "\n",
        "        # 4. Estad√≠sticas por municipio + d√≠a de semana\n",
        "        muni_day_stats = df.groupby(['municipio', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        muni_day_stats.columns = ['municipio', 'dia_semana', 'md_mean', 'md_std']\n",
        "        self.profiles['muni_day'] = muni_day_stats\n",
        "\n",
        "        # 5. Estad√≠sticas globales (fallback)\n",
        "        self.global_stats = {\n",
        "            'mean': df['cantidad'].mean(),\n",
        "            'std': df['cantidad'].std(),\n",
        "            'median': df['cantidad'].median()\n",
        "        }\n",
        "\n",
        "        print(f\"   ‚úì {len(group_stats)} perfiles √∫nicos creados\")\n",
        "        print(f\"   ‚úì Cobertura: {(group_stats['count'].sum() / len(df)) * 100:.1f}%\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features basadas en perfiles hist√≥ricos\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Merge con estad√≠sticas principales\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['main'],\n",
        "            on=self.group_cols,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas l√≠nea-municipio\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['line_muni'],\n",
        "            on=['linea', 'municipio'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas l√≠nea-d√≠a\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['line_day'],\n",
        "            on=['linea', 'dia_semana'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Merge con estad√≠sticas municipio-d√≠a\n",
        "        X_new = X_new.merge(\n",
        "            self.profiles['muni_day'],\n",
        "            on=['municipio', 'dia_semana'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Rellenar valores faltantes con estad√≠sticas globales\n",
        "        fill_cols = ['mean', 'std', 'median', 'lm_mean', 'lm_std', 'lm_median',\n",
        "                     'ld_mean', 'ld_std', 'md_mean', 'md_std']\n",
        "\n",
        "        for col in fill_cols:\n",
        "            if col in X_new.columns:\n",
        "                base_stat = 'mean' if 'mean' in col else 'std' if 'std' in col else 'median'\n",
        "                X_new[col].fillna(self.global_stats[base_stat], inplace=True)\n",
        "\n",
        "        # Features derivadas\n",
        "        X_new['volatility'] = X_new['std'] / (X_new['mean'] + 1)  # Coeficiente de variaci√≥n\n",
        "        X_new['normalized_demand'] = X_new['mean'] / (X_new['lm_mean'] + 1)  # Demanda relativa\n",
        "\n",
        "        return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V_KTlp6BdLIO"
      },
      "outputs": [],
      "source": [
        "class WeatherImpactEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica c√≥mo el clima afecta hist√≥ricamente a cada l√≠nea-municipio.\n",
        "\n",
        "    Ejemplo:\n",
        "    --------\n",
        "    - Algunas l√≠neas son muy sensibles a lluvia (zonas sin alternativas)\n",
        "    - Otras no tanto (zonas c√©ntricas con m√∫ltiples opciones)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weather_impacts = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula sensibilidad al clima por grupo\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüå¶Ô∏è Calculando impacto del clima...\")\n",
        "\n",
        "        for group in df.groupby(['linea', 'municipio']):\n",
        "            key = group[0]\n",
        "            data = group[1]\n",
        "\n",
        "            if len(data) < 10:  # Muy pocos datos\n",
        "                continue\n",
        "\n",
        "            # Correlaci√≥n entre lluvia y demanda\n",
        "            rain_corr = data[['precip', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            # Diferencia de demanda en d√≠as lluviosos vs secos\n",
        "            rainy_days = data[data['precip'] > 5]['cantidad'].mean()\n",
        "            dry_days = data[data['precip'] <= 5]['cantidad'].mean()\n",
        "            rain_impact = (rainy_days - dry_days) / dry_days if dry_days > 0 else 0\n",
        "\n",
        "            # Sensibilidad a temperatura\n",
        "            temp_corr = data[['t_med', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            self.weather_impacts[key] = {\n",
        "                'rain_correlation': rain_corr,\n",
        "                'rain_impact_pct': rain_impact,\n",
        "                'temp_correlation': temp_corr\n",
        "            }\n",
        "\n",
        "        print(f\"   ‚úì {len(self.weather_impacts)} perfiles clim√°ticos creados\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features de impacto clim√°tico\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Inicializar columnas\n",
        "        X_new['rain_sensitivity'] = 0.0\n",
        "        X_new['rain_impact'] = 0.0\n",
        "        X_new['temp_sensitivity'] = 0.0\n",
        "\n",
        "        # Aplicar perfiles\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'])\n",
        "\n",
        "            if key in self.weather_impacts:\n",
        "                impacts = self.weather_impacts[key]\n",
        "                X_new.loc[idx, 'rain_sensitivity'] = impacts['rain_correlation']\n",
        "                X_new.loc[idx, 'rain_impact'] = impacts['rain_impact_pct']\n",
        "                X_new.loc[idx, 'temp_sensitivity'] = impacts['temp_correlation']\n",
        "\n",
        "        # Interacciones clima √ó sensibilidad\n",
        "        X_new['adjusted_rain'] = X_new['precip'] * X_new['rain_sensitivity']\n",
        "        X_new['adjusted_temp'] = X_new['t_med'] * X_new['temp_sensitivity']\n",
        "\n",
        "        return X_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "taIRBEUmdN7Q"
      },
      "outputs": [],
      "source": [
        "class SeasonalityEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Codifica patrones estacionales (mensuales/semanales) por grupo.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.seasonal_patterns = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula factores estacionales\"\"\"\n",
        "\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        print(\"\\nüìÖ Calculando patrones estacionales...\")\n",
        "\n",
        "        # Por l√≠nea-municipio-mes\n",
        "        monthly = df.groupby(['linea', 'municipio', 'mes'])['cantidad'].mean()\n",
        "\n",
        "        # Normalizar por promedio anual de cada grupo\n",
        "        for (linea, municipio) in df.groupby(['linea', 'municipio']).groups.keys():\n",
        "            subset = monthly.loc[linea, municipio]\n",
        "            if len(subset) > 0:\n",
        "                annual_avg = subset.mean()\n",
        "                if annual_avg > 0:\n",
        "                    for mes in subset.index:\n",
        "                        key = (linea, municipio, mes)\n",
        "                        self.seasonal_patterns[key] = subset[mes] / annual_avg\n",
        "\n",
        "        print(f\"   ‚úì {len(self.seasonal_patterns)} patrones estacionales\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega factor estacional\"\"\"\n",
        "\n",
        "        X_new = X.copy()\n",
        "        X_new['seasonal_factor'] = 1.0\n",
        "\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'], row['mes'])\n",
        "            if key in self.seasonal_patterns:\n",
        "                X_new.loc[idx, 'seasonal_factor'] = self.seasonal_patterns[key]\n",
        "\n",
        "        return X_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3_nxl0cbFpiX"
      },
      "outputs": [],
      "source": [
        "class DateSorter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Convierte y ordena por fecha. Debe ser el primer paso del pipeline.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='fecha'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Convertir fecha a datetime si no lo est√°\n",
        "        if not pd.api.types.is_datetime64_any_dtype(X[self.date_column]):\n",
        "            X[self.date_column] = pd.to_datetime(X[self.date_column])\n",
        "\n",
        "        # Ordenar por fecha\n",
        "        X = X.sort_values(self.date_column).reset_index(drop=True)\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fFhPK50DDBUU"
      },
      "outputs": [],
      "source": [
        "class TemporalFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extrae features temporales de la columna fecha y crea features c√≠clicas.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='fecha'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # La fecha ya debe estar como datetime gracias a DateSorter\n",
        "\n",
        "        # Extraer features temporales b√°sicas\n",
        "        X['dia_semana'] = X[self.date_column].dt.dayofweek\n",
        "        X['mes'] = X[self.date_column].dt.month\n",
        "        X['is_weekend'] = X['dia_semana'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "        # Features c√≠clicas para mes\n",
        "        X['mes_sin'] = np.sin(2 * np.pi * X['mes'] / 12)\n",
        "        X['mes_cos'] = np.cos(2 * np.pi * X['mes'] / 12)\n",
        "\n",
        "        # Features c√≠clicas para d√≠a de semana\n",
        "        X['dia_sin'] = np.sin(2 * np.pi * X['dia_semana'] / 7)\n",
        "        X['dia_cos'] = np.cos(2 * np.pi * X['dia_semana'] / 7)\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8q4SEaf3DFSL"
      },
      "outputs": [],
      "source": [
        "class TemperatureFeatureCreator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Crea features derivadas de temperatura.\n",
        "    \"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Temperatura media\n",
        "        X['t_med'] = (X['tmax'] + X['tmin']) / 2\n",
        "\n",
        "        # Amplitud t√©rmica\n",
        "        X['t_amp'] = X['tmax'] - X['tmin']\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D2l07osnKHwQ"
      },
      "outputs": [],
      "source": [
        "class DropNaRows(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Elimina filas que tengan NaN en columnas espec√≠ficas.\n",
        "    Pensado para usarse luego de LagFeatureCreator, antes del split X/y.\n",
        "    \"\"\"\n",
        "    def __init__(self, columns=None, how='any'):\n",
        "        self.columns = columns or []\n",
        "        self.how = how  # 'any' o 'all'\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if self.columns:\n",
        "            X = X.dropna(subset=self.columns, how=self.how)\n",
        "        X = X.reset_index(drop=True)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sasya0rdDH-P"
      },
      "outputs": [],
      "source": [
        "class LagFeatureCreator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Crea features de lag y rolling para series temporales.\n",
        "    Agrupa por l√≠nea y municipio para mantener la coherencia temporal.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_col='cantidad', date_col='fecha',\n",
        "                 group_cols=['linea', 'municipio'],\n",
        "                 lags=[1, 7, 28],\n",
        "                 rolling_windows=[7, 28]):\n",
        "        self.target_col = target_col\n",
        "        self.date_col = date_col\n",
        "        self.group_cols = group_cols\n",
        "        self.lags = lags\n",
        "        self.rolling_windows = rolling_windows\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Asegurar que fecha sea datetime y ordenar\n",
        "        if not pd.api.types.is_datetime64_any_dtype(X[self.date_col]):\n",
        "            X[self.date_col] = pd.to_datetime(X[self.date_col])\n",
        "\n",
        "        X = X.sort_values(self.date_col).reset_index(drop=True)\n",
        "\n",
        "        def add_lags_rolls(g):\n",
        "            g = g.sort_values(self.date_col)\n",
        "\n",
        "            # Crear lags\n",
        "            for lag in self.lags:\n",
        "                g[f'lag_{lag}'] = g[self.target_col].shift(lag)\n",
        "\n",
        "            # Crear rolling means\n",
        "            for window in self.rolling_windows:\n",
        "                g[f'roll_{window}'] = g[self.target_col].shift(1).rolling(\n",
        "                    window=window, min_periods=1\n",
        "                ).mean()\n",
        "\n",
        "            # Indicadores de lag faltante (opcional)\n",
        "            for lag in self.lags:\n",
        "                g[f'has_lag_{lag}'] = g[f'lag_{lag}'].isnull().astype(int)\n",
        "\n",
        "            return g\n",
        "\n",
        "        # Aplicar por grupo\n",
        "        X = X.groupby(self.group_cols, group_keys=False).apply(\n",
        "            add_lags_rolls\n",
        "        )\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J4G_zb0ZDJ5K"
      },
      "outputs": [],
      "source": [
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Elimina columnas espec√≠ficas del DataFrame.\n",
        "    \"\"\"\n",
        "    def __init__(self, columns_to_drop=None):\n",
        "        self.columns_to_drop = columns_to_drop or []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X.drop(columns=self.columns_to_drop, errors='ignore')\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oeaq5FymDSUN"
      },
      "outputs": [],
      "source": [
        "class Winsorizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Aplica winsorizaci√≥n a columnas espec√≠ficas para manejar outliers.\n",
        "    \"\"\"\n",
        "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99, columns=None):\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "        self.columns = columns\n",
        "        self.lower_bounds = {}\n",
        "        self.upper_bounds = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.columns is None:\n",
        "            self.columns = X.columns\n",
        "\n",
        "        for col in self.columns:\n",
        "            if col in X.columns:\n",
        "                self.lower_bounds[col] = np.percentile(\n",
        "                    X[col].dropna(), self.lower_percentile * 100\n",
        "                )\n",
        "                self.upper_bounds[col] = np.percentile(\n",
        "                    X[col].dropna(), self.upper_percentile * 100\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        for col in self.columns:\n",
        "            if col in X_transformed.columns:\n",
        "                X_transformed[col] = np.clip(\n",
        "                    X_transformed[col],\n",
        "                    self.lower_bounds[col],\n",
        "                    self.upper_bounds[col]\n",
        "                )\n",
        "        return X_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HistoricalProfileEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Codifica el perfil hist√≥rico de cada l√≠nea-municipio-d√≠a_semana.\"\"\"\n",
        "\n",
        "    def __init__(self, group_cols=['linea', 'municipio', 'dia_semana']):\n",
        "        self.group_cols = group_cols\n",
        "        self.profiles = {}\n",
        "        self.global_stats = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula perfiles hist√≥ricos desde los datos de entrenamiento\"\"\"\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        # 1. Estad√≠sticas por grupo (l√≠nea + municipio + d√≠a de semana)\n",
        "        group_stats = df.groupby(self.group_cols)['cantidad'].agg([\n",
        "            'mean', 'std', 'median', 'min', 'max', 'count'\n",
        "        ]).reset_index()\n",
        "        self.profiles['main'] = group_stats\n",
        "\n",
        "        # 2. Estad√≠sticas por l√≠nea + municipio\n",
        "        line_muni_stats = df.groupby(['linea', 'municipio'])['cantidad'].agg([\n",
        "            'mean', 'std', 'median'\n",
        "        ]).reset_index()\n",
        "        line_muni_stats.columns = ['linea', 'municipio', 'lm_mean', 'lm_std', 'lm_median']\n",
        "        self.profiles['line_muni'] = line_muni_stats\n",
        "\n",
        "        # 3. Estad√≠sticas por l√≠nea + d√≠a de semana\n",
        "        line_day_stats = df.groupby(['linea', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        line_day_stats.columns = ['linea', 'dia_semana', 'ld_mean', 'ld_std']\n",
        "        self.profiles['line_day'] = line_day_stats\n",
        "\n",
        "        # 4. Estad√≠sticas por municipio + d√≠a de semana\n",
        "        muni_day_stats = df.groupby(['municipio', 'dia_semana'])['cantidad'].agg([\n",
        "            'mean', 'std'\n",
        "        ]).reset_index()\n",
        "        muni_day_stats.columns = ['municipio', 'dia_semana', 'md_mean', 'md_std']\n",
        "        self.profiles['muni_day'] = muni_day_stats\n",
        "\n",
        "        # 5. Estad√≠sticas globales (fallback)\n",
        "        self.global_stats = {\n",
        "            'mean': df['cantidad'].mean(),\n",
        "            'std': df['cantidad'].std(),\n",
        "            'median': df['cantidad'].median()\n",
        "        }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features basadas en perfiles hist√≥ricos\"\"\"\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Merge con estad√≠sticas principales\n",
        "        X_new = X_new.merge(self.profiles['main'], on=self.group_cols, how='left')\n",
        "        X_new = X_new.merge(self.profiles['line_muni'], on=['linea', 'municipio'], how='left')\n",
        "        X_new = X_new.merge(self.profiles['line_day'], on=['linea', 'dia_semana'], how='left')\n",
        "        X_new = X_new.merge(self.profiles['muni_day'], on=['municipio', 'dia_semana'], how='left')\n",
        "\n",
        "        # Rellenar valores faltantes con estad√≠sticas globales\n",
        "        fill_cols = ['mean', 'std', 'median', 'lm_mean', 'lm_std', 'lm_median',\n",
        "                     'ld_mean', 'ld_std', 'md_mean', 'md_std']\n",
        "        for col in fill_cols:\n",
        "            if col in X_new.columns:\n",
        "                base_stat = 'mean' if 'mean' in col else 'std' if 'std' in col else 'median'\n",
        "                X_new[col].fillna(self.global_stats[base_stat], inplace=True)\n",
        "\n",
        "        # Features derivadas\n",
        "        X_new['volatility'] = X_new['std'] / (X_new['mean'] + 1)\n",
        "        X_new['normalized_demand'] = X_new['mean'] / (X_new['lm_mean'] + 1)\n",
        "\n",
        "        return X_new\n",
        "\n",
        "\n",
        "class WeatherImpactEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Codifica c√≥mo el clima afecta hist√≥ricamente a cada l√≠nea-municipio.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.weather_impacts = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula sensibilidad al clima por grupo\"\"\"\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        for group in df.groupby(['linea', 'municipio']):\n",
        "            key = group[0]\n",
        "            data = group[1]\n",
        "\n",
        "            if len(data) < 10:\n",
        "                continue\n",
        "\n",
        "            # Correlaci√≥n entre lluvia y demanda\n",
        "            rain_corr = data[['precip', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            # Diferencia de demanda en d√≠as lluviosos vs secos\n",
        "            rainy_days = data[data['precip'] > 5]['cantidad'].mean()\n",
        "            dry_days = data[data['precip'] <= 5]['cantidad'].mean()\n",
        "            rain_impact = (rainy_days - dry_days) / dry_days if dry_days > 0 else 0\n",
        "\n",
        "            # Sensibilidad a temperatura\n",
        "            temp_corr = data[['t_med', 'cantidad']].corr().iloc[0, 1]\n",
        "\n",
        "            self.weather_impacts[key] = {\n",
        "                'rain_correlation': rain_corr if not np.isnan(rain_corr) else 0.0,\n",
        "                'rain_impact_pct': rain_impact if not np.isnan(rain_impact) else 0.0,\n",
        "                'temp_correlation': temp_corr if not np.isnan(temp_corr) else 0.0\n",
        "            }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega features de impacto clim√°tico\"\"\"\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Inicializar columnas\n",
        "        X_new['rain_sensitivity'] = 0.0\n",
        "        X_new['rain_impact'] = 0.0\n",
        "        X_new['temp_sensitivity'] = 0.0\n",
        "\n",
        "        # Aplicar perfiles\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'])\n",
        "            if key in self.weather_impacts:\n",
        "                impacts = self.weather_impacts[key]\n",
        "                X_new.loc[idx, 'rain_sensitivity'] = impacts['rain_correlation']\n",
        "                X_new.loc[idx, 'rain_impact'] = impacts['rain_impact_pct']\n",
        "                X_new.loc[idx, 'temp_sensitivity'] = impacts['temp_correlation']\n",
        "\n",
        "        # Interacciones clima √ó sensibilidad\n",
        "        X_new['adjusted_rain'] = X_new['precip'] * X_new['rain_sensitivity']\n",
        "        X_new['adjusted_temp'] = X_new['t_med'] * X_new['temp_sensitivity']\n",
        "\n",
        "        return X_new\n",
        "\n",
        "\n",
        "class SeasonalityEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Codifica patrones estacionales (mensuales/semanales) por grupo.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.seasonal_patterns = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Calcula factores estacionales\"\"\"\n",
        "        df = X.copy()\n",
        "        df['cantidad'] = y\n",
        "\n",
        "        # Por l√≠nea-municipio-mes\n",
        "        monthly = df.groupby(['linea', 'municipio', 'mes'])['cantidad'].mean()\n",
        "\n",
        "        # Normalizar por promedio anual de cada grupo\n",
        "        for (linea, municipio) in df.groupby(['linea', 'municipio']).groups.keys():\n",
        "            try:\n",
        "                subset = monthly.loc[linea, municipio]\n",
        "                if len(subset) > 0:\n",
        "                    annual_avg = subset.mean()\n",
        "                    if annual_avg > 0:\n",
        "                        for mes in subset.index:\n",
        "                            key = (linea, municipio, mes)\n",
        "                            self.seasonal_patterns[key] = subset[mes] / annual_avg\n",
        "            except (KeyError, IndexError):\n",
        "                continue\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Agrega factor estacional\"\"\"\n",
        "        X_new = X.copy()\n",
        "        X_new['seasonal_factor'] = 1.0\n",
        "\n",
        "        for idx, row in X_new.iterrows():\n",
        "            key = (row['linea'], row['municipio'], row['mes'])\n",
        "            if key in self.seasonal_patterns:\n",
        "                X_new.loc[idx, 'seasonal_factor'] = self.seasonal_patterns[key]\n",
        "\n",
        "        return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t2HGGqRmKNnx"
      },
      "outputs": [],
      "source": [
        "def create_fe_pipeline(\n",
        "    target_col='cantidad',\n",
        "    date_col='fecha',\n",
        "    group_cols=['linea', 'municipio'],\n",
        "    lags=[1, 7, 28],\n",
        "    rolling_windows=[7, 28],\n",
        "    dropna_lag_cols=('cantidad', 'lag_1', 'lag_7', 'lag_28', 'roll_7', 'roll_28')\n",
        "):\n",
        "    return Pipeline([\n",
        "        (\"date_sorter\", DateSorter(date_column='fecha')),\n",
        "        (\"temporal_features\", TemporalFeatureExtractor(date_column='fecha')),\n",
        "        (\"temperature_features\", TemperatureFeatureCreator()),\n",
        "        (\"historical_profiles\", HistoricalProfileEncoder()),\n",
        "        (\"weather_impact\", WeatherImpactEncoder()),\n",
        "        (\"seasonality\", SeasonalityEncoder())\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wV5xFU_kL4Gl"
      },
      "outputs": [],
      "source": [
        "# Recrear el pipeline final sin lambdas\n",
        "def create_final_preprocessing_pipeline(\n",
        "    columns_to_drop=['fecha', 'provincia', 'nombre_feriado', 'tipo_transporte', 'tipo_feriado', 'linea', 'empresa'],\n",
        "    winsorize_cols=['t_med', 'precip', 'tmax', 'tmin']\n",
        "):\n",
        "    \"\"\"\n",
        "    Versi√≥n serializable del pipeline final.\n",
        "    Usa make_column_selector en lugar de lambdas.\n",
        "    \"\"\"\n",
        "    numeric_pipeline = Pipeline([\n",
        "        (\"winsorizer\", Winsorizer(columns=winsorize_cols)),\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"drop_columns\", DropColumns(columns_to_drop=columns_to_drop)),\n",
        "        (\"column_transform\", ColumnTransformer([\n",
        "            (\"num\", numeric_pipeline, make_column_selector(dtype_include=[\"int64\", \"float64\"])),\n",
        "            (\"cat\", categorical_pipeline, make_column_selector(dtype_include=[\"object\", \"category\"]))\n",
        "        ]))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsUuzCZBIBYk"
      },
      "source": [
        "## 1. Objetivo predictivo del proyecto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iGnm8lxIFAR"
      },
      "source": [
        "- **Objetivo del proyecto**:  Predecir la cantidad de pasajeros transportados por una l√≠nea de colectivo en una fecha determinada, utilizando informaci√≥n contextual como clima, feriados, ubicaci√≥n y caracter√≠sticas del servicio.\n",
        "\n",
        "  - **Variable objetivo**: `cantidad` (n√∫mero entero que representa la cantidad de pasajeros)\n",
        "\n",
        "  - **Tipo de problema**: `Regresi√≥n`, ya que se busca predecir un valor num√©rico continuo.\n",
        "\n",
        "- **Justificaci√≥n**: La variable `cantidad` no representa clases ni categor√≠as, sino un conteo que var√≠a en funci√≥n de m√∫ltiples factores. Por lo tanto, se trata de un problema de regresi√≥n supervisada.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLsCywfYpjde"
      },
      "source": [
        "## 2. An√°lisis del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW6aKxdQ7cJc",
        "outputId": "d666bae7-a90f-48df-d233-4cf73dfd3eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- (FILAS, COLUMNAS) -----\n",
            "(133204, 14)\n",
            "---- (TIPOS DE DATOS) -----\n",
            "fecha               object\n",
            "empresa             object\n",
            "linea               object\n",
            "tipo_transporte     object\n",
            "provincia           object\n",
            "municipio           object\n",
            "cantidad             int64\n",
            "tmax               float64\n",
            "tmin               float64\n",
            "precip             float64\n",
            "viento             float64\n",
            "is_feriado           int64\n",
            "tipo_feriado        object\n",
            "nombre_feriado      object\n",
            "dtype: object\n",
            "---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\n",
            "count    133204.000000\n",
            "mean      14055.787386\n",
            "std       19946.489535\n",
            "min           1.000000\n",
            "25%        2169.000000\n",
            "50%        6973.000000\n",
            "75%       17665.000000\n",
            "max      189636.000000\n",
            "Name: cantidad, dtype: float64\n",
            "---- (NULOS) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia              22\n",
            "municipio              22\n",
            "cantidad                0\n",
            "tmax                 2137\n",
            "tmin                 2137\n",
            "precip               2137\n",
            "viento               2137\n",
            "is_feriado              0\n",
            "tipo_feriado       126287\n",
            "nombre_feriado     126287\n",
            "dtype: int64\n",
            "---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\n",
            "(131067, 14)\n",
            "---- (NULOS DESPU√âS DE DROPEAR NAN) -----\n",
            "fecha                   0\n",
            "empresa                 0\n",
            "linea                   0\n",
            "tipo_transporte         0\n",
            "provincia               0\n",
            "municipio               0\n",
            "cantidad                0\n",
            "tmax                    0\n",
            "tmin                    0\n",
            "precip                  0\n",
            "viento                  0\n",
            "is_feriado              0\n",
            "tipo_feriado       124210\n",
            "nombre_feriado     124210\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('final_2024-11-04.csv')\n",
        "print(\"---- (FILAS, COLUMNAS) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (TIPOS DE DATOS) -----\")\n",
        "print(df.dtypes)\n",
        "print(\"---- (ESTAD√çSTICOS VARIABLE OBJETIVO) -----\")\n",
        "print(df['cantidad'].describe())\n",
        "print(\"---- (NULOS) -----\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with NaN in specified columns\n",
        "df = df.dropna(subset=['tmax', 'tmin', 'precip', 'viento'], how='any').reset_index(drop=True)\n",
        "\n",
        "print(\"---- (FILAS, COLUMNAS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.shape)\n",
        "print(\"---- (NULOS DESPU√âS DE DROPEAR NAN) -----\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNkE4hcMppD7"
      },
      "source": [
        "###  2.1 Preprocesamiento y pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6HV76Hz_Sn_"
      },
      "source": [
        "**MUY IMPORTANTE ESTO** Se obtiene el dia y el mes de la semana, porque la fecha como tal no sirve para el entrenamiento, no se puede generalizar.\n",
        "\n",
        "Debemos obtener tambi√©n el n√∫mero de d√≠a en la semana (0 - 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZpPyQHHK9LQY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('final_2024-11-04.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J2OpOcauIrNa"
      },
      "outputs": [],
      "source": [
        "preprocessor = create_fe_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1MB7STb5rfY",
        "outputId": "50987a9d-9bd3-42f9-e2c7-3f38f2c21d38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ignac\\AppData\\Local\\Temp\\ipykernel_15892\\243475955.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_new[col].fillna(self.global_stats[base_stat], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Filtrar NaN en columnas cr√≠ticas ANTES de separar\n",
        "# Primero aplicar solo los transformadores temporales b√°sicos para generar dia_semana\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pipeline m√≠nimo para generar dia_semana y mes\n",
        "temp_pipeline = Pipeline([\n",
        "    (\"date_sorter\", DateSorter(date_column='fecha')),\n",
        "    (\"temporal_features\", TemporalFeatureExtractor(date_column='fecha'))\n",
        "])\n",
        "\n",
        "# Aplicar temporalmente para verificar columnas\n",
        "df_temp = temp_pipeline.fit_transform(df)\n",
        "\n",
        "# Filtrar filas donde las columnas de agrupaci√≥n tienen NaN\n",
        "critical_cols = ['linea', 'municipio', 'dia_semana', 'mes']\n",
        "mask = df_temp[critical_cols].notna().all(axis=1)\n",
        "df_clean = df_temp[mask].reset_index(drop=True)\n",
        "\n",
        "# Ahora separar y aplicar pipeline completo\n",
        "X = df_clean.drop(columns=['cantidad'])\n",
        "y = df_clean['cantidad'].reset_index(drop=True)\n",
        "\n",
        "# Aplicar feature engineering con y\n",
        "df_fe = preprocessor.fit_transform(X, y)\n",
        "\n",
        "# Asegurar que df_fe es DataFrame y tiene el √≠ndice correcto\n",
        "if not isinstance(df_fe, pd.DataFrame):\n",
        "    # Si es array numpy, convertir a DataFrame\n",
        "    df_fe = pd.DataFrame(df_fe, index=X.index)\n",
        "\n",
        "# Agregar columnas necesarias para el split temporal\n",
        "df_fe['fecha'] = df_clean['fecha'].values\n",
        "df_fe['cantidad'] = y.values\n",
        "\n",
        "# Resetear √≠ndice para asegurar consistencia\n",
        "df_fe = df_fe.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoOSDySqKkgw",
        "outputId": "837a3e68-35f5-457b-cf25-b1d86a131023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Train: 93348 muestras, Valid: 19935 muestras, Test: 19899 muestras\n"
          ]
        }
      ],
      "source": [
        "# 4) Split temporal 70/15/15 (la columna 'fecha' ya es datetime y est√° ordenada)\n",
        "# Verificar que df_fe tenga la columna fecha\n",
        "if 'fecha' not in df_fe.columns:\n",
        "    raise ValueError(\"La columna 'fecha' no est√° en df_fe. Verifica el pipeline de feature engineering.\")\n",
        "\n",
        "# Verificar que df_fe tenga la columna cantidad\n",
        "if 'cantidad' not in df_fe.columns:\n",
        "    raise ValueError(\"La columna 'cantidad' no est√° en df_fe. Verifica el pipeline de feature engineering.\")\n",
        "\n",
        "# Asegurar que fecha es datetime\n",
        "if not pd.api.types.is_datetime64_any_dtype(df_fe['fecha']):\n",
        "    df_fe['fecha'] = pd.to_datetime(df_fe['fecha'])\n",
        "\n",
        "# Ordenar por fecha\n",
        "df_fe = df_fe.sort_values('fecha').reset_index(drop=True)\n",
        "\n",
        "cut_train = df_fe[\"fecha\"].quantile(0.70)\n",
        "cut_valid = df_fe[\"fecha\"].quantile(0.85)\n",
        "\n",
        "train_mask = df_fe[\"fecha\"] <= cut_train\n",
        "valid_mask = (df_fe[\"fecha\"] > cut_train) & (df_fe[\"fecha\"] <= cut_valid)\n",
        "test_mask  = df_fe[\"fecha\"] > cut_valid\n",
        "\n",
        "df_train = df_fe[train_mask].copy()\n",
        "df_valid = df_fe[valid_mask].copy()\n",
        "df_test  = df_fe[test_mask].copy()\n",
        "\n",
        "# 5) Separar X e y (reci√©n ahora sacamos la target de X)\n",
        "X_train = df_train.drop(columns=[\"cantidad\"])\n",
        "y_train = df_train[\"cantidad\"]\n",
        "X_valid = df_valid.drop(columns=[\"cantidad\"])\n",
        "y_valid = df_valid[\"cantidad\"]\n",
        "X_test  = df_test.drop(columns=[\"cantidad\"])\n",
        "y_test  = df_test[\"cantidad\"]\n",
        "\n",
        "# Verificar que las separaciones fueron exitosas\n",
        "print(f\"‚úì Train: {len(X_train)} muestras, Valid: {len(X_valid)} muestras, Test: {len(X_test)} muestras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hw_42c7oLuJA"
      },
      "outputs": [],
      "source": [
        "# 5) Preprocesamiento final (ONEHOT/SCALER) reci√©n ahora\n",
        "final_pre = create_final_preprocessing_pipeline()\n",
        "X_train_transformed = final_pre.fit_transform(X_train)\n",
        "X_valid_transformed = final_pre.transform(X_valid)\n",
        "X_test_transformed  = final_pre.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnnQxmPUp0YV"
      },
      "source": [
        "## 3. Comparaci√≥n de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y2kB8fFp3V4"
      },
      "source": [
        "### 3.1 Evaluaci√≥n con m√©tricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "R6TvWvw6HKJN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MOBScn-2HQ7A"
      },
      "outputs": [],
      "source": [
        "def safe_mape(y_true, y_pred, eps=1e-6):\n",
        "    \"\"\"Calcula MAPE de forma segura evitando divisi√≥n por cero\"\"\"\n",
        "    denom = np.where(np.abs(y_true) < eps, eps, y_true)\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, dataset_name):\n",
        "    \"\"\"Calcula todas las m√©tricas relevantes\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = safe_mape(np.array(y_true), np.array(y_pred))\n",
        "\n",
        "    return {\n",
        "        'Dataset': dataset_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R¬≤': r2,\n",
        "        'MAPE (%)': mape\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSsn415sHiMo",
        "outputId": "783423e3-3bff-4a8a-ff3b-6e616e0eee93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Modelos a comparar:\n",
            "   1. Linear Regression (Baseline)\n",
            "   2. Random Forest\n",
            "   3. Gradient Boosting\n",
            "   4. XGBoost (con configuraci√≥n optimizada)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìã Modelos a comparar:\")\n",
        "print(\"   1. Linear Regression (Baseline)\")\n",
        "print(\"   2. Random Forest\")\n",
        "print(\"   3. Gradient Boosting\")\n",
        "print(\"   4. XGBoost (con configuraci√≥n optimizada)\")\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    'XGBoost': XGBRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.01,\n",
        "        max_depth=12,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=10,\n",
        "        random_state=42,\n",
        "        early_stopping_rounds=50,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3GLPHoNHvgY",
        "outputId": "c31914d8-4de1-433e-be45-bb4dec34b8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÑ Entrenando y evaluando modelos...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "‚ñ∂ Entrenando Linear Regression...\n",
            "  ‚úì Entrenamiento completado en 0.50 segundos\n",
            "  üìä Valid RMSE: 18319.70 | R¬≤: 0.0706\n",
            "\n",
            "‚ñ∂ Entrenando Random Forest...\n",
            "  ‚úì Entrenamiento completado en 18.50 segundos\n",
            "  üìä Valid RMSE: 18722.40 | R¬≤: 0.0293\n",
            "\n",
            "‚ñ∂ Entrenando Gradient Boosting...\n",
            "  ‚úì Entrenamiento completado en 76.51 segundos\n",
            "  üìä Valid RMSE: 18467.46 | R¬≤: 0.0555\n",
            "\n",
            "‚ñ∂ Entrenando XGBoost...\n",
            "  ‚úì Entrenamiento completado en 12.47 segundos\n",
            "  üìä Valid RMSE: 18409.91 | R¬≤: 0.0614\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîÑ Entrenando y evaluando modelos...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "all_results = []\n",
        "training_times = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n‚ñ∂ Entrenando {name}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    if name == 'XGBoost':\n",
        "        # XGBoost con early stopping\n",
        "        model.fit(\n",
        "            X_train_transformed,\n",
        "            y_train,\n",
        "            eval_set=[(X_train_transformed, y_train), (X_valid_transformed, y_valid)],\n",
        "            verbose=False\n",
        "        )\n",
        "    else:\n",
        "        model.fit(X_train_transformed, y_train)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    training_times[name] = training_time\n",
        "\n",
        "    print(f\"  ‚úì Entrenamiento completado en {training_time:.2f} segundos\")\n",
        "\n",
        "    # Generar predicciones\n",
        "    y_train_pred = model.predict(X_train_transformed)\n",
        "    y_valid_pred = model.predict(X_valid_transformed)\n",
        "    y_test_pred = model.predict(X_test_transformed)\n",
        "\n",
        "    # Calcular m√©tricas para cada conjunto\n",
        "    metrics_train = calculate_metrics(y_train, y_train_pred, f'{name} - Train')\n",
        "    metrics_valid = calculate_metrics(y_valid, y_valid_pred, f'{name} - Valid')\n",
        "    metrics_test = calculate_metrics(y_test, y_test_pred, f'{name} - Test')\n",
        "\n",
        "    # Agregar a resultados\n",
        "    all_results.extend([metrics_train, metrics_valid, metrics_test])\n",
        "\n",
        "    # Mostrar m√©tricas del modelo\n",
        "    print(f\"  üìä Valid RMSE: {metrics_valid['RMSE']:.2f} | R¬≤: {metrics_valid['R¬≤']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0zipR8JIKFn",
        "outputId": "73cd75f4-6aba-4647-f5b4-f4c3fc59b9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTADOS COMPLETOS - TODOS LOS MODELOS\n",
            "================================================================================\n",
            "                  Dataset         RMSE          MAE       R¬≤    MAPE (%)\n",
            "Linear Regression - Train 19277.685428 12024.182984 0.086441 1636.053187\n",
            "Linear Regression - Valid 18319.700951 11836.663851 0.070597 2209.662424\n",
            " Linear Regression - Test 19178.387815 12566.411406 0.060363 1332.716364\n",
            "    Random Forest - Train 18546.574849 11587.123830 0.154421 1482.746199\n",
            "    Random Forest - Valid 18722.404475 12018.734381 0.029288 2504.265890\n",
            "     Random Forest - Test 19325.785218 12623.918918 0.045864 1328.925987\n",
            "Gradient Boosting - Train 19113.684506 11841.175038 0.101918 1568.849821\n",
            "Gradient Boosting - Valid 18467.460900 11885.823999 0.055544 2338.342128\n",
            " Gradient Boosting - Test 19221.306131 12582.810445 0.056153 1331.625238\n",
            "          XGBoost - Train 17915.266339 11055.517578 0.211006 1539.668278\n",
            "          XGBoost - Valid 18409.907767 11801.648438 0.061422 2325.997362\n",
            "           XGBoost - Test 19267.210696 12460.058594 0.051639 1364.313790\n"
          ]
        }
      ],
      "source": [
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS COMPLETOS - TODOS LOS MODELOS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25-O6FdXIKz5",
        "outputId": "71bab526-ffde-4e3f-c48a-c5b9d8581131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARACI√ìN RESUMIDA (VALIDATION SET)\n",
            "================================================================================\n",
            "           Modelo         RMSE          MAE       R¬≤    MAPE (%)\n",
            "Linear Regression 18319.700951 11836.663851 0.070597 2209.662424\n",
            "          XGBoost 18409.907767 11801.648438 0.061422 2325.997362\n",
            "Gradient Boosting 18467.460900 11885.823999 0.055544 2338.342128\n",
            "    Random Forest 18722.404475 12018.734381 0.029288 2504.265890\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN RESUMIDA (VALIDATION SET)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "valid_results = results_df[results_df['Dataset'].str.contains('Valid')].copy()\n",
        "valid_results['Modelo'] = valid_results['Dataset'].str.replace(' - Valid', '')\n",
        "valid_results = valid_results[['Modelo', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)']].sort_values('RMSE')\n",
        "\n",
        "print(valid_results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2sPzMckINho",
        "outputId": "ceeab37d-a4f3-4e0f-df4f-6bddb2eafda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÜ MEJOR MODELO: Linear Regression\n",
            "   RMSE: 18319.70\n",
            "   R¬≤: 0.0706\n",
            "   Tiempo de entrenamiento: 0.50 segundos\n"
          ]
        }
      ],
      "source": [
        "best_model_name = valid_results.iloc[0]['Modelo']\n",
        "best_rmse = valid_results.iloc[0]['RMSE']\n",
        "best_r2 = valid_results.iloc[0]['R¬≤']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   RMSE: {best_rmse:.2f}\")\n",
        "print(f\"   R¬≤: {best_r2:.4f}\")\n",
        "print(f\"   Tiempo de entrenamiento: {training_times[best_model_name]:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBBPj6eANRI3"
      },
      "source": [
        "# CONCLUSI√ìN DEL MODELO QUE VAMOS A USAR\n",
        "\n",
        "Vamos a usar el modelo de RANDOM FOREST por el MAPE, ya que es muy inferior al de XGBOOST.\n",
        "\n",
        "Si bien, el XGBoost tiene mejores RMSE y R2 (predice mejor casos donde el error es costoso) su MAPE es muy alto, por lo que falla mucho en errores t√≠picos.\n",
        "\n",
        "En cambio, Random Forest tiene un RMSE un poco menor, pero tiene mejor predicci√≥n ante errores t√≠picos, por lo que se lo considera m√°s \"estable\".\n",
        "\n",
        "## POST AJUSTES OVERFITTING / UNDERFITTING\n",
        "\n",
        "Despu√©s de pruebas de overfitting/underfitting se ajustaron campos como el max_depth del Random forest para que finalmente est√© balanceado y adem√°s sea el mejor modelo. El XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ7-QYVKIPqd",
        "outputId": "a76fbda6-b906-4b3d-f1b4-b15fb345586b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DIAGN√ìSTICO DE OVERFITTING - Linear Regression\n",
            "================================================================================\n",
            "üìä M√âTRICAS:\n",
            "   Train RMSE: 19277.69 | R¬≤: 0.0864\n",
            "   Valid RMSE: 18319.70 | R¬≤: 0.0706\n",
            "   Test  RMSE: 19178.39  | R¬≤: 0.0604\n",
            "\n",
            "üîç AN√ÅLISIS:\n",
            "   Diferencia Train-Valid RMSE: +957.98\n",
            "   Diferencia Valid-Train R¬≤: -0.0158\n",
            "\n",
            "   ‚ùå UNDERFITTING SEVERO\n",
            "   üí° Soluciones:\n",
            "      - Aumentar complejidad del modelo\n",
            "      - Reducir regularizaci√≥n\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"DIAGN√ìSTICO DE OVERFITTING - {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_train_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Train'].iloc[0]\n",
        "best_valid_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Valid'].iloc[0]\n",
        "best_test_metrics = results_df[results_df['Dataset'] == f'{best_model_name} - Test'].iloc[0]\n",
        "\n",
        "train_rmse = best_train_metrics['RMSE']\n",
        "valid_rmse = best_valid_metrics['RMSE']\n",
        "test_rmse = best_test_metrics['RMSE']\n",
        "\n",
        "train_r2 = best_train_metrics['R¬≤']\n",
        "valid_r2 = best_valid_metrics['R¬≤']\n",
        "test_r2 = best_test_metrics['R¬≤']\n",
        "\n",
        "overfit_rmse = train_rmse - valid_rmse\n",
        "overfit_r2 = valid_r2 - train_r2\n",
        "\n",
        "print(f\"üìä M√âTRICAS:\")\n",
        "print(f\"   Train RMSE: {train_rmse:.2f} | R¬≤: {train_r2:.4f}\")\n",
        "print(f\"   Valid RMSE: {valid_rmse:.2f} | R¬≤: {valid_r2:.4f}\")\n",
        "print(f\"   Test  RMSE: {test_rmse:.2f}  | R¬≤: {test_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nüîç AN√ÅLISIS:\")\n",
        "print(f\"   Diferencia Train-Valid RMSE: {overfit_rmse:+.2f}\")\n",
        "print(f\"   Diferencia Valid-Train R¬≤: {overfit_r2:+.4f}\")\n",
        "\n",
        "# Diagn√≥stico autom√°tico\n",
        "if overfit_rmse < -500:\n",
        "    print(\"\\n   ‚ùå OVERFITTING SEVERO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar regularizaci√≥n\")\n",
        "    print(\"      - Reducir learning_rate\")\n",
        "    print(\"      - Reducir max_depth\")\n",
        "elif overfit_rmse < 0:\n",
        "    print(\"\\n   ‚ö†Ô∏è OVERFITTING MODERADO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar ligeramente la regularizaci√≥n\")\n",
        "elif overfit_rmse < 200:\n",
        "    print(\"\\n   ‚úÖ BIEN BALANCEADO\")\n",
        "elif overfit_rmse < 500:\n",
        "    print(\"\\n   ‚ö†Ô∏è LIGERO UNDERFITTING\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar max_depth\")\n",
        "    print(\"      - Aumentar learning_rate\")\n",
        "else:\n",
        "    print(\"\\n   ‚ùå UNDERFITTING SEVERO\")\n",
        "    print(\"   üí° Soluciones:\")\n",
        "    print(\"      - Aumentar complejidad del modelo\")\n",
        "    print(\"      - Reducir regularizaci√≥n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8fbfUKsVVj4",
        "outputId": "b7b4a2f6-52ee-4592-ca09-6024a65d6229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GUARDANDO ARTEFACTOS\n",
            "================================================================================\n",
            "\n",
            "üì¶ Guardando fe_pipeline...\n",
            "üì¶ Guardando final_preprocessor...\n",
            "üì¶ Guardando modelo...\n",
            "\n",
            "‚úÖ Artefactos guardados exitosamente en 'artifacts/'\n",
            "\n",
            "üìã Archivos generados:\n",
            "   - fe_pipeline.joblib (feature engineering)\n",
            "   - preprocessor.joblib (encoding + scaling)\n",
            "   - model.joblib (Linear Regression)\n",
            "   - metadata.json (configuraci√≥n)\n",
            "\n",
            "üîç Verificando carga de artefactos...\n",
            "   ‚úÖ Feature engineering pipeline cargado\n",
            "   ‚úÖ Preprocessor cargado\n",
            "   ‚úÖ Modelo Linear Regression cargado\n",
            "   ‚úÖ Metadata cargada\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EXPORTACI√ìN COMPLETADA\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os, json, joblib, sklearn\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "# IMPORTANTE: Guardar el modelo entrenado, no el modelo sin entrenar\n",
        "# Los modelos ya fueron entrenados en el loop anterior, pero necesitamos guardar el mejor\n",
        "# Re-entrenar el mejor modelo para asegurarnos de que est√© entrenado\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"GUARDANDO ARTEFACTOS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Re-entrenar el mejor modelo con los datos completos\n",
        "best_model = models[best_model_name]\n",
        "if best_model_name == 'XGBoost':\n",
        "    best_model.fit(\n",
        "        X_train_transformed,\n",
        "        y_train,\n",
        "        eval_set=[(X_train_transformed, y_train), (X_valid_transformed, y_valid)],\n",
        "        verbose=False\n",
        "    )\n",
        "else:\n",
        "    best_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "ARTIFACT_DIR = \"artifacts\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üì¶ Guardando fe_pipeline...\")\n",
        "joblib.dump(preprocessor, f\"{ARTIFACT_DIR}/fe_pipeline.joblib\")\n",
        "\n",
        "print(\"üì¶ Guardando final_preprocessor...\")\n",
        "joblib.dump(final_pre, f\"{ARTIFACT_DIR}/preprocessor.joblib\")\n",
        "\n",
        "print(\"üì¶ Guardando modelo...\")\n",
        "joblib.dump(best_model, f\"{ARTIFACT_DIR}/model.joblib\")\n",
        "\n",
        "# Metadata\n",
        "meta = {\n",
        "    \"model_name\": best_model_name,\n",
        "    \"date_col\": \"fecha\",\n",
        "    \"target_col\": \"cantidad\",\n",
        "    \"requires_history\": False,\n",
        "    \"dropped_cols\": [\"fecha\", \"provincia\", \"nombre_feriado\", \"tipo_transporte\", \"tipo_feriado\", \"linea\", \"empresa\"],\n",
        "    \"winsorize_cols\": [\"t_med\", \"precip\", \"tmax\", \"tmin\"],\n",
        "    \"sklearn_version\": sklearn.__version__,\n",
        "    \"metrics\": {\n",
        "        \"valid_rmse\": best_rmse,\n",
        "        \"valid_r2\": best_r2,\n",
        "        \"training_time_seconds\": training_times[best_model_name]\n",
        "    }\n",
        "}\n",
        "json.dump(meta, open(f\"{ARTIFACT_DIR}/metadata.json\", \"w\"), indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Artefactos guardados exitosamente en '{ARTIFACT_DIR}/'\")\n",
        "print(f\"\\nüìã Archivos generados:\")\n",
        "print(f\"   - fe_pipeline.joblib (feature engineering)\")\n",
        "print(f\"   - preprocessor.joblib (encoding + scaling)\")\n",
        "print(f\"   - model.joblib ({best_model_name})\")\n",
        "print(f\"   - metadata.json (configuraci√≥n)\")\n",
        "\n",
        "# Verificaci√≥n: cargar y probar\n",
        "print(f\"\\nüîç Verificando carga de artefactos...\")\n",
        "fe_loaded = joblib.load(f\"{ARTIFACT_DIR}/fe_pipeline.joblib\")\n",
        "prep_loaded = joblib.load(f\"{ARTIFACT_DIR}/preprocessor.joblib\")\n",
        "model_loaded = joblib.load(f\"{ARTIFACT_DIR}/model.joblib\")\n",
        "meta_loaded = json.load(open(f\"{ARTIFACT_DIR}/metadata.json\"))\n",
        "\n",
        "print(f\"   ‚úÖ Feature engineering pipeline cargado\")\n",
        "print(f\"   ‚úÖ Preprocessor cargado\")\n",
        "print(f\"   ‚úÖ Modelo {meta_loaded['model_name']} cargado\")\n",
        "print(f\"   ‚úÖ Metadata cargada\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"‚úÖ EXPORTACI√ìN COMPLETADA\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgDK7svup7Eg"
      },
      "source": [
        "## 4. Ajuste de hiperpar√°metros\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFbj6mbku1M8",
        "outputId": "3ebb9e70-ff9c-4f26-d955-6a300375ca9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "AJUSTE DE HIPERPAR√ÅMETROS - RANDOM FOREST\n",
            "================================================================================\n",
            "\n",
            "üìã Definiendo espacio de b√∫squeda...\n",
            "‚úì Espacio de b√∫squeda definido:\n",
            "   - n_estimators: 3 opciones\n",
            "   - max_depth: 3 opciones\n",
            "   - min_samples_split: 3 opciones\n",
            "   - min_samples_leaf: 3 opciones\n",
            "   - max_features: 3 opciones\n",
            "   - bootstrap: 2 opciones\n",
            "   - max_samples: 3 opciones\n"
          ]
        }
      ],
      "source": [
        "## 4. Ajuste de Hiperpar√°metros - Random Forest\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AJUSTE DE HIPERPAR√ÅMETROS - RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 4.1 Definir espacio de b√∫squeda de hiperpar√°metros\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìã Definiendo espacio de b√∫squeda...\")\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 300],           # N√∫mero de √°rboles\n",
        "    'max_depth': [10, 15, 20],            # Profundidad m√°xima\n",
        "    'min_samples_split': [2, 5, 10],            # M√≠nimo para split\n",
        "    'min_samples_leaf': [1, 2, 4],               # M√≠nimo en hojas\n",
        "    'max_features': ['sqrt', 'log2', 0.5],     # Features por split\n",
        "    'bootstrap': [True, False],                      # Muestreo con reemplazo\n",
        "    'max_samples': [0.7, 0.8, 0.9]            # % de muestras por √°rbol\n",
        "}\n",
        "\n",
        "print(\"‚úì Espacio de b√∫squeda definido:\")\n",
        "for param, values in param_distributions.items():\n",
        "    print(f\"   - {param}: {len(values)} opciones\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYAZWA2zu9F1",
        "outputId": "2c4c08dc-f1ee-4b01-cd07-d011bbce46af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Configurando b√∫squeda aleatoria...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nüîç Configurando b√∫squeda aleatoria...\")\n",
        "\n",
        "# Guardar m√©tricas del modelo anterior para comparaci√≥n\n",
        "modelo_anterior_valid_rmse = best_valid_metrics['RMSE']\n",
        "modelo_anterior_valid_r2 = best_valid_metrics['R¬≤']\n",
        "modelo_anterior_test_rmse = best_test_metrics['RMSE']\n",
        "modelo_anterior_test_r2 = best_test_metrics['R¬≤']\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=30,                          # N√∫mero de combinaciones a probar\n",
        "    cv=3,                               # 3-fold cross-validation\n",
        "    scoring='neg_root_mean_squared_error',  # M√©trica de optimizaci√≥n\n",
        "    n_jobs=-1,                          # Usar todos los cores\n",
        "    random_state=42,\n",
        "    verbose=2,                          # Mostrar progreso\n",
        "    return_train_score=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEJHwrsUV4Ny",
        "outputId": "b70d69b4-a5cc-43b7-892e-90aa3ff9527a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Iniciando b√∫squeda de hiperpar√°metros...\n",
            "   (Esto puede tomar varios minutos...)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   (Esto puede tomar varios minutos...)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m search_time = time.time() - start_time\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ B√∫squeda completada en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutos (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m segundos)\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ignac\\Documents\\Repos\\sube-G17\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ Iniciando b√∫squeda de hiperpar√°metros...\")\n",
        "print(\"   (Esto puede tomar varios minutos...)\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "random_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "search_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ B√∫squeda completada en {search_time/60:.2f} minutos ({search_time:.2f} segundos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d_4M1l_V-EK",
        "outputId": "efe77142-2641-476d-859e-bd30b3018a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTADOS DE LA B√öSQUEDA\n",
            "================================================================================\n",
            "\n",
            "üèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\n",
            "   n_estimators: 200\n",
            "   min_samples_split: 5\n",
            "   min_samples_leaf: 2\n",
            "   max_samples: 0.9\n",
            "   max_features: sqrt\n",
            "   max_depth: 10\n",
            "   bootstrap: True\n",
            "\n",
            "üìä Mejor RMSE en Cross-Validation: 19425.63\n",
            "\n",
            "üîù TOP 5 MEJORES CONFIGURACIONES:\n",
            "   RMSE (CV)   Std RMSE  Tiempo (s)\n",
            "19425.629893 147.541823   35.828893\n",
            "19432.851653 134.962801   58.278489\n",
            "19434.461211 142.856511  204.420633\n",
            "19445.344082 141.523967   64.119635\n",
            "19448.167087 143.641039   13.777914\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS DE LA B√öSQUEDA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\")\n",
        "for param, value in random_search.best_params_.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "# Convertir score negativo a RMSE positivo\n",
        "best_cv_rmse = -random_search.best_score_\n",
        "print(f\"\\nüìä Mejor RMSE en Cross-Validation: {best_cv_rmse:.2f}\")\n",
        "\n",
        "# Top 5 mejores combinaciones\n",
        "print(\"\\nüîù TOP 5 MEJORES CONFIGURACIONES:\")\n",
        "results_df = pd.DataFrame(random_search.cv_results_)\n",
        "results_df['mean_test_rmse'] = -results_df['mean_test_score']\n",
        "results_df = results_df.sort_values('mean_test_rmse')\n",
        "\n",
        "top_5 = results_df.head(5)[['mean_test_rmse', 'std_test_score', 'mean_fit_time']]\n",
        "top_5.columns = ['RMSE (CV)', 'Std RMSE', 'Tiempo (s)']\n",
        "print(top_5.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPo5o_0WFEG",
        "outputId": "e1433f85-d856-42d9-bdba-41bab9bdbbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ENTRENAMIENTO DEL MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "\n",
            "üîÑ Entrenando Random Forest con hiperpar√°metros optimizados...\n",
            "‚úì Modelo entrenado en 26.30 segundos\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENTRENAMIENTO DEL MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîÑ Entrenando Random Forest con hiperpar√°metros optimizados...\")\n",
        "\n",
        "final_rf_model = RandomForestRegressor(\n",
        "    **random_search.best_params_,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start_train = time.time()\n",
        "final_rf_model.fit(X_train_transformed, y_train)\n",
        "train_time = time.time() - start_train\n",
        "\n",
        "print(f\"‚úì Modelo entrenado en {train_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKjBp6ZXWXPj",
        "outputId": "28667f3c-0eb3-4dc1-ad78-3a4bf0d35800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Generando predicciones...\n",
            "\n",
            "================================================================================\n",
            "M√âTRICAS DEL MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "           Dataset         RMSE          MAE       R¬≤    MAPE (%)\n",
            "Train (Optimizado) 18804.535962 11675.858427 0.130735 1555.245606\n",
            "Valid (Optimizado) 18394.479884 11876.224113 0.062994 2327.672414\n",
            " Test (Optimizado) 19217.017361 12457.660252 0.056574 1312.163760\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä Generando predicciones...\")\n",
        "\n",
        "y_train_pred_opt = final_rf_model.predict(X_train_transformed)\n",
        "y_valid_pred_opt = final_rf_model.predict(X_valid_transformed)\n",
        "y_test_pred_opt = final_rf_model.predict(X_test_transformed)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "metrics_train_opt = calculate_metrics(y_train, y_train_pred_opt, 'Train (Optimizado)')\n",
        "metrics_valid_opt = calculate_metrics(y_valid, y_valid_pred_opt, 'Valid (Optimizado)')\n",
        "metrics_test_opt = calculate_metrics(y_test, y_test_pred_opt, 'Test (Optimizado)')\n",
        "\n",
        "results_opt_df = pd.DataFrame([metrics_train_opt, metrics_valid_opt, metrics_test_opt])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"M√âTRICAS DEL MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "print(results_opt_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCDS-crGWZKJ",
        "outputId": "d31e58d2-fa14-46d8-8010-f545146b3add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARACI√ìN: MODELO ANTERIOR vs MODELO OPTIMIZADO\n",
            "================================================================================\n",
            "\n",
            "Conjunto  RMSE Anterior  RMSE Optimizado  R¬≤ Anterior  R¬≤ Optimizado  Mejora RMSE  Mejora R¬≤\n",
            "   Train   19277.685428     18804.535962     0.086441       0.130735   473.149466   0.044294\n",
            "   Valid   18319.700951     18394.479884     0.070597       0.062994   -74.778934  -0.007603\n",
            "    Test   19178.387815     19217.017361     0.060363       0.056574   -38.629546  -0.003789\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN: MODELO ANTERIOR vs MODELO OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_data = {\n",
        "    'Conjunto': ['Train', 'Valid', 'Test'],\n",
        "    'RMSE Anterior': [\n",
        "        best_train_metrics['RMSE'],\n",
        "        modelo_anterior_valid_rmse,\n",
        "        modelo_anterior_test_rmse\n",
        "    ],\n",
        "    'RMSE Optimizado': [\n",
        "        metrics_train_opt['RMSE'],\n",
        "        metrics_valid_opt['RMSE'],\n",
        "        metrics_test_opt['RMSE']\n",
        "    ],\n",
        "    'R¬≤ Anterior': [\n",
        "        best_train_metrics['R¬≤'],\n",
        "        modelo_anterior_valid_r2,\n",
        "        modelo_anterior_test_r2\n",
        "    ],\n",
        "    'R¬≤ Optimizado': [\n",
        "        metrics_train_opt['R¬≤'],\n",
        "        metrics_valid_opt['R¬≤'],\n",
        "        metrics_test_opt['R¬≤']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Mejora RMSE'] = comparison_df['RMSE Anterior'] - comparison_df['RMSE Optimizado']\n",
        "comparison_df['Mejora R¬≤'] = comparison_df['R¬≤ Optimizado'] - comparison_df['R¬≤ Anterior']\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXDb8TiLWkuq",
        "outputId": "1be7eb9c-a89f-4dfd-b0a9-0782b86ecca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "AN√ÅLISIS DE MEJORAS\n",
            "================================================================================\n",
            "\n",
            "üìà MEJORAS EN VALIDATION SET:\n",
            "   Mejora RMSE: -74.78 (-0.41%)\n",
            "   Mejora R¬≤: -0.0076\n",
            "\n",
            "   ‚ö†Ô∏è El modelo anterior era mejor o similar\n",
            "   üí° Considerar:\n",
            "      - El modelo base ya estaba bien ajustado\n",
            "      - Posible overfitting en la b√∫squeda de hiperpar√°metros\n",
            "\n",
            "üîç DIAGN√ìSTICO DE OVERFITTING:\n",
            "   Diferencia Train-Valid RMSE: +410.06\n",
            "   ‚ö†Ô∏è UNDERFITTING LEVE: El modelo podr√≠a ser m√°s complejo\n",
            "\n",
            "üìä CONSISTENCIA VALID-TEST:\n",
            "   Diferencia RMSE: 822.54\n",
            "   ‚ö†Ô∏è Hay diferencia significativa entre Valid y Test\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISIS DE MEJORAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mejora_rmse_valid = modelo_anterior_valid_rmse - metrics_valid_opt['RMSE']\n",
        "mejora_r2_valid = metrics_valid_opt['R¬≤'] - modelo_anterior_valid_r2\n",
        "mejora_porcentual = (mejora_rmse_valid / modelo_anterior_valid_rmse) * 100\n",
        "\n",
        "print(f\"\\nüìà MEJORAS EN VALIDATION SET:\")\n",
        "print(f\"   Mejora RMSE: {mejora_rmse_valid:+.2f} ({mejora_porcentual:+.2f}%)\")\n",
        "print(f\"   Mejora R¬≤: {mejora_r2_valid:+.4f}\")\n",
        "\n",
        "if mejora_rmse_valid > 50:\n",
        "    print(\"\\n   üèÜ EXCELENTE: Mejora significativa con la optimizaci√≥n\")\n",
        "elif mejora_rmse_valid > 10:\n",
        "    print(\"\\n   ‚úÖ BUENO: Mejora notable con la optimizaci√≥n\")\n",
        "elif mejora_rmse_valid > 0:\n",
        "    print(\"\\n   ‚úì POSITIVO: Ligera mejora con la optimizaci√≥n\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è El modelo anterior era mejor o similar\")\n",
        "    print(\"   üí° Considerar:\")\n",
        "    print(\"      - El modelo base ya estaba bien ajustado\")\n",
        "    print(\"      - Posible overfitting en la b√∫squeda de hiperpar√°metros\")\n",
        "\n",
        "# An√°lisis de overfitting\n",
        "overfit_rmse_opt = metrics_train_opt['RMSE'] - metrics_valid_opt['RMSE']\n",
        "\n",
        "print(f\"\\nüîç DIAGN√ìSTICO DE OVERFITTING:\")\n",
        "print(f\"   Diferencia Train-Valid RMSE: {overfit_rmse_opt:+.2f}\")\n",
        "\n",
        "if abs(overfit_rmse_opt) < 200:\n",
        "    print(\"   ‚úÖ BIEN BALANCEADO: Modelo optimizado bien generalizado\")\n",
        "elif overfit_rmse_opt < 0:\n",
        "    print(\"   ‚ö†Ô∏è OVERFITTING DETECTADO: El modelo memoriza datos de train\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è UNDERFITTING LEVE: El modelo podr√≠a ser m√°s complejo\")\n",
        "\n",
        "# Consistencia Valid-Test\n",
        "valid_test_diff = abs(metrics_valid_opt['RMSE'] - metrics_test_opt['RMSE'])\n",
        "print(f\"\\nüìä CONSISTENCIA VALID-TEST:\")\n",
        "print(f\"   Diferencia RMSE: {valid_test_diff:.2f}\")\n",
        "\n",
        "if valid_test_diff < 300:\n",
        "    print(\"   ‚úÖ EXCELENTE: Performance consistente\")\n",
        "elif valid_test_diff < 500:\n",
        "    print(\"   ‚úÖ BUENO: Performance aceptable\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Hay diferencia significativa entre Valid y Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tIo-WHr9rb2"
      },
      "source": [
        "## 6. Predicci√≥n final (ejemplo de una fecha futura)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uZUu_nhovOfV",
        "outputId": "b24d888d-caa4-4090-d634-69ed9a94119c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PREDICCI√ìN FINAL - EJEMPLO PR√ÅCTICO\n",
            "================================================================================\n",
            "\n",
            "üìã EJEMPLO 1: PREDICCI√ìN CON DATOS REALES DEL TEST SET\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìÖ INFORMACI√ìN DE LA PREDICCI√ìN:\n",
            "   Fecha: 2024-11-28 (Thursday)\n",
            "   L√≠nea: BSAS_LINEA_506\n",
            "   Municipio: florencio varela\n",
            "   Empresa: TRANSPORTES SAN JUAN BAUTISTA S.A.\n",
            "\n",
            "üå§Ô∏è CONDICIONES CLIM√ÅTICAS:\n",
            "   Temperatura m√°xima: 27.3¬∞C\n",
            "   Temperatura m√≠nima: 17.7¬∞C\n",
            "   Temperatura media: 22.5¬∞C\n",
            "   Precipitaci√≥n: 1.0 mm\n",
            "   Viento: 22.9 km/h\n",
            "\n",
            "üìä CONTEXTO TEMPORAL:\n",
            "   D√≠a de la semana: Jueves\n",
            "   Es fin de semana: No\n",
            "   Mes: 11\n",
            "   Es feriado: No\n",
            "\n",
            "üìä PERFILES HIST√ìRICOS:\n",
            "   (Los perfiles hist√≥ricos se calculan autom√°ticamente durante la predicci√≥n)\n",
            "   El modelo utiliza estad√≠sticas hist√≥ricas de pasajeros por:\n",
            "   - L√≠nea + Municipio + D√≠a de la semana\n",
            "   - L√≠nea + Municipio\n",
            "   - L√≠nea + D√≠a de la semana\n",
            "   - Municipio + D√≠a de la semana\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'std'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'std'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3347439815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Preprocesar y predecir (usar ambos pipelines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msample_data_fe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0msample_data_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_fe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_rf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3631316190.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Features derivadas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volatility'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Coeficiente de variaci√≥n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_demand'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lm_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Demanda relativa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'std'"
          ]
        }
      ],
      "source": [
        "## 6. Predicci√≥n Final - Ejemplo Pr√°ctico\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICCI√ìN FINAL - EJEMPLO PR√ÅCTICO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.1 Ejemplo con datos reales del Test Set\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìã EJEMPLO 1: PREDICCI√ìN CON DATOS REALES DEL TEST SET\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Tomar una muestra aleatoria del test set\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "sample_data_raw = X_test.iloc[sample_idx:sample_idx+1].copy()\n",
        "\n",
        "# Obtener fecha correspondiente del dataframe del test set\n",
        "sample_date_info = df_test.iloc[sample_idx]\n",
        "\n",
        "print(\"\\nüìÖ INFORMACI√ìN DE LA PREDICCI√ìN:\")\n",
        "print(f\"   Fecha: {sample_date_info['fecha'].strftime('%Y-%m-%d (%A)')}\")\n",
        "print(f\"   L√≠nea: {sample_data_raw['linea'].iloc[0]}\")\n",
        "print(f\"   Municipio: {sample_data_raw['municipio'].iloc[0]}\")\n",
        "print(f\"   Empresa: {sample_data_raw['empresa'].iloc[0]}\")\n",
        "\n",
        "print(f\"\\nüå§Ô∏è CONDICIONES CLIM√ÅTICAS:\")\n",
        "print(f\"   Temperatura m√°xima: {sample_data_raw['tmax'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Temperatura m√≠nima: {sample_data_raw['tmin'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Temperatura media: {sample_data_raw['t_med'].iloc[0]:.1f}¬∞C\")\n",
        "print(f\"   Precipitaci√≥n: {sample_data_raw['precip'].iloc[0]:.1f} mm\")\n",
        "print(f\"   Viento: {sample_data_raw['viento'].iloc[0]:.1f} km/h\")\n",
        "\n",
        "print(f\"\\nüìä CONTEXTO TEMPORAL:\")\n",
        "dia_nombres = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']\n",
        "dia_semana_num = int(sample_data_raw['dia_semana'].iloc[0])\n",
        "print(f\"   D√≠a de la semana: {dia_nombres[dia_semana_num]}\")\n",
        "print(f\"   Es fin de semana: {'S√≠' if sample_data_raw['is_weekend'].iloc[0] else 'No'}\")\n",
        "print(f\"   Mes: {sample_data_raw['mes'].iloc[0]}\")\n",
        "print(f\"   Es feriado: {'S√≠' if sample_data_raw['is_feriado'].iloc[0] else 'No'}\")\n",
        "\n",
        "print(f\"\\nüìä PERFILES HIST√ìRICOS:\")\n",
        "print(\"   (Los perfiles hist√≥ricos se calculan autom√°ticamente durante la predicci√≥n)\")\n",
        "print(\"   El modelo utiliza estad√≠sticas hist√≥ricas de pasajeros por:\")\n",
        "print(\"   - L√≠nea + Municipio + D√≠a de la semana\")\n",
        "print(\"   - L√≠nea + Municipio\")\n",
        "print(\"   - L√≠nea + D√≠a de la semana\")\n",
        "print(\"   - Municipio + D√≠a de la semana\")\n",
        "\n",
        "# Preprocesar y predecir (usar ambos pipelines)\n",
        "sample_data_fe = preprocessor.transform(sample_data_raw)\n",
        "sample_data_transformed = final_pre.transform(sample_data_fe)\n",
        "prediction = final_rf_model.predict(sample_data_transformed)[0]\n",
        "actual = y_test.iloc[sample_idx]\n",
        "\n",
        "print(f\"\\nüéØ RESULTADO DE LA PREDICCI√ìN:\")\n",
        "print(f\"   Predicci√≥n del modelo: {prediction:.0f} pasajeros\")\n",
        "print(f\"   Valor real observado: {actual:.0f} pasajeros\")\n",
        "print(f\"   Error absoluto: {abs(prediction - actual):.0f} pasajeros\")\n",
        "\n",
        "# Evaluaci√≥n cualitativa del error\n",
        "error_pct = abs(prediction - actual) / actual * 100\n",
        "if error_pct < 5:\n",
        "    print(\"   ‚úÖ Excelente: Error menor al 5%\")\n",
        "elif error_pct < 10:\n",
        "    print(\"   ‚úÖ Muy bueno: Error menor al 10%\")\n",
        "elif error_pct < 20:\n",
        "    print(\"   ‚úì Aceptable: Error menor al 20%\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Alto: Error mayor al 20%\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.2 Funci√≥n para predicciones futuras\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FUNCI√ìN DE PREDICCI√ìN FUTURA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_future_passengers(linea, municipio, empresa, fecha, tmax, tmin, precip, viento,\n",
        "                              lag_1=None, lag_7=None, lag_28=None, is_feriado=0, tipo_feriado=None):\n",
        "    \"\"\"\n",
        "    Predice la cantidad de pasajeros para una fecha futura.\n",
        "\n",
        "    Par√°metros:\n",
        "    -----------\n",
        "    linea : str\n",
        "        C√≥digo de la l√≠nea de colectivo (ej: 'BSAS_LINEA_740')\n",
        "    municipio : str\n",
        "        Nombre del municipio (ej: 'san miguel')\n",
        "    empresa : str\n",
        "        Nombre de la empresa operadora\n",
        "    fecha : str\n",
        "        Fecha en formato 'YYYY-MM-DD'\n",
        "    tmax : float\n",
        "        Temperatura m√°xima esperada (¬∞C)\n",
        "    tmin : float\n",
        "        Temperatura m√≠nima esperada (¬∞C)\n",
        "    precip : float\n",
        "        Precipitaci√≥n esperada (mm)\n",
        "    viento : float\n",
        "        Velocidad del viento esperada (km/h)\n",
        "    lag_1 : float, optional\n",
        "        Pasajeros del d√≠a anterior (si no se provee, usa promedio hist√≥rico)\n",
        "    lag_7 : float, optional\n",
        "        Pasajeros hace 7 d√≠as (si no se provee, usa promedio hist√≥rico)\n",
        "    lag_28 : float, optional\n",
        "        Pasajeros hace 28 d√≠as (si no se provee, usa promedio hist√≥rico)\n",
        "    is_feriado : int, optional\n",
        "        1 si es feriado, 0 si no lo es (default: 0)\n",
        "    tipo_feriado : str, optional\n",
        "        Tipo de feriado si corresponde\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "    float\n",
        "        Cantidad estimada de pasajeros\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # ESTO SE EVITA CON EL PIPELINE DE LA PARTE DEL PRINCIPIO!!!\n",
        "    # Convertir fecha a datetime\n",
        "    fecha_dt = pd.to_datetime(fecha)\n",
        "\n",
        "    # Calcular features temporales\n",
        "    dia_semana = fecha_dt.dayofweek\n",
        "    mes = fecha_dt.month\n",
        "    is_weekend = 1 if dia_semana >= 5 else 0\n",
        "\n",
        "    # Features c√≠clicas\n",
        "    mes_sin = np.sin(2 * np.pi * mes / 12)\n",
        "    mes_cos = np.cos(2 * np.pi * mes / 12)\n",
        "    dia_sin = np.sin(2 * np.pi * dia_semana / 7)\n",
        "    dia_cos = np.cos(2 * np.pi * dia_semana / 7)\n",
        "\n",
        "    # Features de temperatura\n",
        "    t_med = (tmax + tmin) / 2\n",
        "    t_amp = tmax - tmin\n",
        "\n",
        "    # Valores por defecto para lags si no se proveen\n",
        "    # Usar promedio general del training set\n",
        "    if lag_1 is None:\n",
        "        lag_1 = y_train.mean()\n",
        "    if lag_7 is None:\n",
        "        lag_7 = y_train.mean()\n",
        "    if lag_28 is None:\n",
        "        lag_28 = y_train.mean()\n",
        "\n",
        "    roll_7 = lag_1  # Aproximaci√≥n\n",
        "    roll_28 = lag_7  # Aproximaci√≥n\n",
        "\n",
        "    # Crear DataFrame con todos los features\n",
        "    future_data = pd.DataFrame({\n",
        "        'empresa': [empresa],\n",
        "        'linea': [linea],\n",
        "        'municipio': [municipio],\n",
        "        'tmax': [tmax],\n",
        "        'tmin': [tmin],\n",
        "        'precip': [precip],\n",
        "        'viento': [viento],\n",
        "        'is_feriado': [is_feriado],\n",
        "        'tipo_feriado': [tipo_feriado],\n",
        "        'dia_semana': [dia_semana],\n",
        "        'mes': [mes],\n",
        "        'is_weekend': [is_weekend],\n",
        "        'mes_sin': [mes_sin],\n",
        "        'mes_cos': [mes_cos],\n",
        "        'dia_sin': [dia_sin],\n",
        "        'dia_cos': [dia_cos],\n",
        "        't_med': [t_med],\n",
        "        't_amp': [t_amp],\n",
        "        'lag_1': [lag_1],\n",
        "        'lag_7': [lag_7],\n",
        "        'lag_28': [lag_28],\n",
        "        'roll_7': [roll_7],\n",
        "        'roll_28': [roll_28],\n",
        "        'has_lag_1': [0],\n",
        "        'has_lag_7': [0],\n",
        "        'has_lag_28': [0]\n",
        "    })\n",
        "\n",
        "      # Aplicar feature engineering primero\n",
        "    future_data_fe = preprocessor.transform(future_data)\n",
        "      # Luego aplicar preprocesamiento final\n",
        "    future_data_transformed = final_pre.transform(future_data_fe)\n",
        "    prediction = final_rf_model.predict(future_data_transformed)[0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "print(\"\\n‚úÖ Funci√≥n 'predict_future_passengers' creada\")\n",
        "print(\"\\nüìù Par√°metros de la funci√≥n:\")\n",
        "print(\"   - linea: C√≥digo de l√≠nea (obligatorio)\")\n",
        "print(\"   - municipio: Municipio (obligatorio)\")\n",
        "print(\"   - empresa: Empresa operadora (obligatorio)\")\n",
        "print(\"   - fecha: Fecha YYYY-MM-DD (obligatorio)\")\n",
        "print(\"   - tmax, tmin: Temperaturas en ¬∞C (obligatorio)\")\n",
        "print(\"   - precip: Precipitaci√≥n en mm (obligatorio)\")\n",
        "print(\"   - viento: Velocidad en km/h (obligatorio)\")\n",
        "print(\"   - lag_1, lag_7, lag_28: Datos hist√≥ricos (opcional)\")\n",
        "print(\"   - is_feriado: 0 o 1 (opcional, default: 0)\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.3 Ejemplos de uso de la funci√≥n\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EJEMPLOS DE PREDICCIONES FUTURAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ejemplo 1: D√≠a laboral normal\n",
        "print(\"\\nüìù EJEMPLO 1: D√çA LABORAL NORMAL (Mi√©rcoles)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo1 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-15\",  # Mi√©rcoles\n",
        "    tmax=28.0,\n",
        "    tmin=20.0,\n",
        "    precip=0.0,\n",
        "    viento=15.0,\n",
        "    lag_1=15000,  # Ayer hubo 15,000 pasajeros\n",
        "    lag_7=14500,  # Hace una semana hubo 14,500\n",
        "    lag_28=14000, # Hace un mes hubo 14,000\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-15 (Mi√©rcoles)\")\n",
        "print(f\"   Condiciones: Buen clima, sin lluvia\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo1:.0f} pasajeros\")\n",
        "\n",
        "# Ejemplo 2: Fin de semana\n",
        "print(\"\\nüìù EJEMPLO 2: FIN DE SEMANA (Domingo)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo2 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-19\",  # Domingo\n",
        "    tmax=30.0,\n",
        "    tmin=22.0,\n",
        "    precip=0.0,\n",
        "    viento=10.0,\n",
        "    lag_1=8000,   # Los domingos hay menos pasajeros\n",
        "    lag_7=14500,\n",
        "    lag_28=14000,\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-19 (Domingo)\")\n",
        "print(f\"   Condiciones: D√≠a soleado\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo2:.0f} pasajeros\")\n",
        "print(f\"   Nota: Menor que d√≠a laboral (esperado para fin de semana)\")\n",
        "\n",
        "# Ejemplo 3: D√≠a con lluvia\n",
        "print(\"\\nüìù EJEMPLO 3: D√çA LLUVIOSO (Lunes)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo3 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-01-20\",  # Lunes\n",
        "    tmax=22.0,\n",
        "    tmin=18.0,\n",
        "    precip=25.0,  # Lluvia intensa\n",
        "    viento=30.0,  # Viento fuerte\n",
        "    lag_1=8000,   # Domingo (menos pasajeros)\n",
        "    lag_7=14500,\n",
        "    lag_28=14000,\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-01-20 (Lunes)\")\n",
        "print(f\"   Condiciones: Lluvia intensa, viento fuerte\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo3:.0f} pasajeros\")\n",
        "print(f\"   Nota: El clima puede afectar la demanda\")\n",
        "\n",
        "# Ejemplo 4: Feriado\n",
        "print(\"\\nüìù EJEMPLO 4: D√çA FERIADO\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo4 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-05-01\",  # 1 de Mayo (D√≠a del Trabajador)\n",
        "    tmax=25.0,\n",
        "    tmin=18.0,\n",
        "    precip=0.0,\n",
        "    viento=12.0,\n",
        "    lag_1=12000,\n",
        "    lag_7=14000,\n",
        "    lag_28=14500,\n",
        "    is_feriado=1,  # Es feriado\n",
        "    tipo_feriado=\"inamovible\"\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-05-01 (D√≠a del Trabajador)\")\n",
        "print(f\"   Condiciones: Feriado nacional\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo4:.0f} pasajeros\")\n",
        "print(f\"   Nota: Los feriados t√≠picamente tienen menos demanda\")\n",
        "\n",
        "# Ejemplo 5: Sin datos hist√≥ricos (usando promedios)\n",
        "print(\"\\nüìù EJEMPLO 5: SIN DATOS HIST√ìRICOS (Nueva ruta o predicci√≥n lejana)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "pred_ejemplo5 = predict_future_passengers(\n",
        "    linea=\"BSAS_LINEA_740\",\n",
        "    municipio=\"san miguel\",\n",
        "    empresa=\"LA PRIMERA DE GRAND BOURG S.A.\",\n",
        "    fecha=\"2025-06-15\",  # Fecha lejana\n",
        "    tmax=20.0,\n",
        "    tmin=12.0,\n",
        "    precip=0.0,\n",
        "    viento=18.0,\n",
        "    # No se proveen lags, usa promedios autom√°ticamente\n",
        "    is_feriado=0\n",
        ")\n",
        "\n",
        "print(f\"   Fecha: 2025-06-15\")\n",
        "print(f\"   Condiciones: Sin datos hist√≥ricos recientes\")\n",
        "print(f\"   Predicci√≥n: {pred_ejemplo5:.0f} pasajeros\")\n",
        "print(f\"   Nota: Basado en promedios hist√≥ricos generales\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 6.4 Resumen y recomendaciones\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN Y RECOMENDACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° C√ìMO USAR LA FUNCI√ìN DE PREDICCI√ìN:\")\n",
        "print(\"   1. Preparar datos de entrada (l√≠nea, municipio, empresa)\")\n",
        "print(\"   2. Obtener pron√≥stico clim√°tico para la fecha\")\n",
        "print(\"   3. Si es posible, incluir datos hist√≥ricos recientes (lags)\")\n",
        "print(\"   4. Indicar si la fecha es feriado\")\n",
        "print(\"   5. La funci√≥n retorna la cantidad estimada de pasajeros\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è CONSIDERACIONES IMPORTANTES:\")\n",
        "print(\"   - Las predicciones son m√°s precisas con datos hist√≥ricos recientes\")\n",
        "print(\"   - El clima puede afectar significativamente la demanda\")\n",
        "print(\"   - Los fines de semana y feriados tienen patrones diferentes\")\n",
        "print(\"   - Para rutas nuevas, la precisi√≥n ser√° menor\")\n",
        "\n",
        "print(\"\\n‚úÖ MODELO LISTO PARA PRODUCCI√ìN\")\n",
        "print(\"   El modelo ha sido entrenado, validado y est√° listo para:\")\n",
        "print(\"   - Predicciones en tiempo real\")\n",
        "print(\"   - Planificaci√≥n de servicios\")\n",
        "print(\"   - Optimizaci√≥n de frecuencias\")\n",
        "print(\"   - An√°lisis de demanda futura\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHHnqxH9xMT"
      },
      "source": [
        "## 7. Conclusiones del trabajo"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
